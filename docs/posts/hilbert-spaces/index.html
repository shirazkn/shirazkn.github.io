<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Hilbert Spaces | Shiraz</title>
<meta name="keywords" content="Linear Algebra">
<meta name="description" content="Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&rsquo;t have er&hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors.">
<meta name="author" content="">
<link rel="canonical" href="https://shirazkn.github.io/posts/hilbert-spaces/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.97cf87836fe9febd6762ee021598f461858ae137854a39219ecf9bf3543a96fa.css" integrity="sha256-l8&#43;Hg2/p/r1nYu4CFZj0YYWK4TeFSjkhns&#43;b81Q6lvo=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js" integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG&#43;9vmJ0cTS&#43;ovo0FeA="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://shirazkn.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shirazkn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shirazkn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shirazkn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shirazkn.github.io/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          
          
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          
          throwOnError : false
        });
    });
</script>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IM+Fell+DW+Pica"><meta property="og:title" content="Hilbert Spaces" />
<meta property="og:description" content="Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&rsquo;t have er&hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://shirazkn.github.io/posts/hilbert-spaces/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-04-21T12:09:09-04:00" />
<meta property="article:modified_time" content="2023-04-21T12:09:09-04:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Hilbert Spaces"/>
<meta name="twitter:description" content="Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&rsquo;t have er&hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://shirazkn.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Hilbert Spaces",
      "item": "https://shirazkn.github.io/posts/hilbert-spaces/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Hilbert Spaces",
  "name": "Hilbert Spaces",
  "description": "Let $\\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\\langle \\cdot, \\cdot\\rangle _\\mathcal X$) and that it is complete, i.e., it doesn\u0026rsquo;t have er\u0026hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors.",
  "keywords": [
    "Linear Algebra"
  ],
  "articleBody": " Let $\\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\\langle \\cdot, \\cdot\\rangle _\\mathcal X$) and that it is complete, i.e., it doesnâ€™t have erâ€¦ holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors. Mathematicians sometimes use â€˜Hilbert spaceâ€™ to refer specifically to infinite-dimensional inner product spaces, but for our purposes, we will conflate the usage of â€˜Hilbert spaceâ€™ to include the finite-dimensional case.\nSome interesting Hilbert spaces are given below. Try to think about what the induced norm $\\lVert x \\rVert_{\\mathcal X}=\\sqrt{\\langle x, x\\rangle}_{\\mathcal X}$ represents in each of these spaces:\nHilbert Space $\\mathcal X$ Inner Product $\\langle \\cdot, \\cdot\\rangle _\\mathcal X$ Numbers $a$, $b\\in \\mathbb R$ $ab$ Random Variables $X,Y\\in \\mathbb R$ $\\mathbb E \\left[XY \\right]$ Real Vectors $x$, $y\\in \\mathbb R^n$ $x^\\intercal y$ Complex Vectors $x$, $y\\in \\mathbb C^n$ $x^\\dagger y$ Matrices $A$, $B\\in \\mathbb R^{m\\times n}$ $\\ \\text{Trace}(A^{\\intercal}B)$ Sequences $(x_i)_{i=1}^{\\infty}, (y_i)_{i=1}^{\\infty}\\in \\ell ^2(\\mathbb R) \\ $ $\\sum_{i=1}^{\\infty} x_i y_i$ Square-Integrable Functions $f,g \\in L^2(\\mathbb R)$ $\\int_{-\\infty}^{\\infty}f(x)\\overline{g(x)}dx$ Here, $\\overline{({}\\cdot{})}$ is the conjugate of a complex number (where you replace $i$ with $-i$), $({}\\cdot{})^\\dagger =\\overline{({}\\cdot{})}^\\intercal $ is called the conjugate transpose of a complex vector.\nProjections A defining feature of Hilbert Spaces is their geometry. Because they have notions of angles , we have\n\\[0 \\leq {|\\langle x, y\\rangle_{\\mathcal X} |} \\leq {\\|x\\|\\|y\\|}\\] where if the first equality holds we say $x$ and $y$ are orthogonal to each other, and the second equality holds if and only if they are linearly dependent: $x = a y$ for $a \\in \\mathbb R$.\nWe can define the projection of an element $x\\in \\mathcal X$ onto a subspace $S\\subseteq \\mathcal X$:\n\\[ \\text{P}_S(x) = \\argmin_{y\\in S}\\|x-y\\|_{\\mathcal X}\\] where $\\lVert v \\rVert_{\\mathcal X} = \\sqrt{\\langle v, v\\rangle}_{\\mathcal X}$ is the induced norm. The remarkable by-product of these definitions of angles, orthogonality, and projection, is that it is consistent with our Euclidean intuition: $x-\\text{P}_S(x)$ is always orthogonal to $S$. A related theorem says that $\\text P_{\\bar S}(x)$ is well-defined and unique even if $\\tilde S$ is any closed convex set in $\\mathcal X$, although in this case we do not have orthogonality of $x-\\text{P}_{\\tilde S}(x)$ to the other elements in $\\tilde S$.\nAs an example, in the Hilbert space of random variables, for its elements $X$ and $Y$, orthogonality corresponds to $\\mathbb E\\left[X Y\\right] = 0$, the norm becomes the variance, and (orthogonal) projection gives the least-squares estimator of given a random variable . More generally, projection can be used to best approximate an element of a Hilbert space using a finite set of basis vectors of that (or another) Hilbert space.\nThe $(\\mathbb R^N, \\lVert{}\\cdot{}\\rVert_2)$ Space of Sequences This looks just like the Euclidean space of $N$-dimensional vectors. I mean, a vector is a set of $N$ coefficients (real numbers) where each coefficient describes the vectorâ€™s distance along the corresponding standard basis vector (or â€˜axisâ€™, if you will).\nNow, letâ€™s ignore all of that and our geometric intuition of vectors, and consider that $\\mathbb R^N = \\mathbb R \\times \\mathbb R \\times \\dots \\times \\mathbb R$ is just a set-theoretic definition. An element $x$ of this space is a sequence of real numbers,\n\\[x = (x_1, x_2, \\dots, x_N)\\] and the norm can be defined as $\\left(\\sum_i^N x_i^2\\right)^{1/2}$. So, nowhere did we have to talk about vectors or matrices. We can generalize this to $N\\rightarrow \\infty$.\nThe $\\ell^2$ Space of Sequences $\\ell^2(\\mathbb R)$ consists of countable sequences of real numbers. â€˜Countableâ€™ here means that we can count them like we can count the natural numbers, but they are infinitely long nonetheless. We denote this sequence as $(x_i)_{i=1}^{\\infty}$. A sequence is in $\\ell^2$ if and only if it is square-summable, which means\n\\[ \\sum_{i=1}^{\\infty}|x_i|^2 \u003c \\infty \\]\nA sequence which is not in $\\ell^2$ (i.e., an â€˜infinite-length vectorâ€™) is $\\left(\\frac{1}{\\sqrt{1}}, \\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{3}}, \\dots\\right)$ because its sum-of-squares decays too slowly as the number of terms increases. A sequence that is in $\\ell ^2$ is $\\left(\\frac{1}{1}, \\frac{1}{2}, \\frac{1}{3}, \\dots\\right)$.\n(Separable) Hilbert Spaces are Isomorphic Isomorpshisms are maps from one type of mathematical object to another that preserve its structure. All (separable) infinite-dimensional Hilbert spaces are isomorphic to the $\\ell^2$ space , which is a fancy way of saying that we can do the following: We first construct a countable orthonormal basis for $\\mathcal X$, denoted as $(e_i)_{i=1}^{\\infty}$. Then, we define an isometric isomorphism (a distance-preserving, structure-preserving mapping) from $\\mathcal X$ to $\\ell^2$, as follows:\n\\[T: \\mathcal X \\rightarrow \\ell^2\\] \\[T(x) = (\\langle e_i,x \\rangle_{\\mathcal X})_{i=1}^{\\infty} \\] which is the set of coefficients of $x$ along each basis vector. Note that $T$ is basis-dependent, we could pick a different basis and get a different $T$. Since $T$ is an isomorphism (i.e., does not â€˜destroyâ€™ information during the mapping), we may hope to be able to go back from $\\ell^2$ to $\\mathcal X$:\n\\[T^*: \\ell^2 \\rightarrow \\mathcal X\\] \\[T^{*}\\left((c_i)_{i=1}^{\\infty}\\right) = \\sum_{i=1}^{\\infty} c_i e_i \\] Here, $T^{*}$ is called the adjoint of $T$; it is an operator that satisfies\n\\[\\langle y,T(x)\\rangle _{\\ell^2} = \\langle T^*(y),x\\rangle _{\\mathcal X}\\]\nIt can be shown that the map $T({}\\cdot{})$ is necessarily linear , and that $T^*$ is also the inverse of $T$ . The discussion thus far should be reminding us of linear transformations in undergraduate linear algebra. Indeed, the mapping $T({}\\cdot{})$ can always be represented via a matrix, although this matrix can be infinite dimensional in general.\nIn the finite-dimensional case, we can just use the Gram-Schmidt process to construct an orthonormal basis. Using a similar reasoning as above, we see that all (separable) Hilbert spaces with dimension $N$ are isomorphic to the $(\\mathbb R^N, \\lVert{}\\cdot{}\\rVert_2)$ space. The condition that $T^*=T^{-1}$ might remind you of unitary matrices ($U^\\dagger = U^{-1}$, where $U^\\dagger$ is the conjugate transpose of $U$), which are exactly the distance-preserving, structure-preserving matrices in $\\mathbb C^N$.\nWhen weâ€™re working in $\\mathbb R^N$, we replace â€˜unitaryâ€™ with â€˜orthonormalâ€™, and we have $Q^\\intercal = Q^{-1}$ for an orthonormal matrix $Q$. They are an isometry (distance-preserving) because $\\lVert Qx\\rVert_2 = \\lVert x \\rVert_2$, and they are an isomorphism (structure-preserving) because $\\langle Qx, Qy \\rangle =\\langle x, Q^\\intercal Qy \\rangle = \\langle x, y\\rangle$.\nThus, we always have a (non-unique) unitary matrix (or a unitary operator , in the infinite-dimensional case) which will take us from one vector space to â€˜anotherâ€™ (but actually, it takes us to a rotated version of the same space ðŸ™ƒ) in a distance-preserving, structure-preserving manner.\nThe $L^p$ Space of Functions Letâ€™s move onwards to function spaces. The space $L^1(\\mathbb R)$ is the space of absolutely integrable functions:\n\\[ \\| f\\|_{L^1} =\\int_{\\mathbb R} | f(x)| dx \u003c \\infty \\]\nwhere $f:\\mathbb R\\rightarrow \\mathbb C$ (so yeah, its codomain can be complex-valued). It is not a Hilbert space, because we have not yet defined an inner product for this space. The space $L^2(\\mathbb R)$ has functions which are square-integrable:\n\\[ \\| f\\|_{L^2}^2 =\\int_{\\mathbb R} | f(x)|^2dx \u003c \\infty \\]\nwhich is a Hilbert space because it has the following inner product + norm combination:\n\\[ \\langle f, g \\rangle_{L^2} = \\int_{\\mathbb R} f(x) \\overline{g(x)} dx\\] \\[ \\| f\\|_{L^2} = \\sqrt{ \\langle f, f \\rangle_{L^2}}\\] where $\\overline {z}$ is the complex conjugate of $z\\in \\mathbb C$. Can we use the above as an inner product for $L^1$ as well? It turns out that we canâ€™t, because even if $f$ is in $L^1$, the integral of â€˜$f(x)f(x)$â€™ can be unbounded if $f$ is not also in $L^2$.\nObserve that we always require some sort of 'boundedness of norm' when defining Hilbert spaces... it is related to the requirement of completeness of the space, which we so conveniently glossed over. Observe that the norms on $\\ell^p$ and $L^p$ spaces are natural extensions of the $p$-norms for finite-dimensional vector spaces, and as expected, we have an inner product only when $p=2$. The $L^p$ space can be thought of as a â€˜refinementâ€™ of the domain of the $\\ell^p$ space, where the index set $\\lbrace 1, 2, \\dots, \\infty\\rbrace$ (which was countable) of $\\ell^p$ is replaced with $\\mathbb R$ (which is uncountable) in $L^p$. So even though these might seem like completely different concepts thrown together, they are closely related and inherit much of each otherâ€™s properties!\nBut since $L^2$ and $\\ell^2$ are supposed to be isomorphic, this suggests that we could go from $L^2$ to $\\ell^2$ using a linear map, which might seem bizarre at first â€“ we would be representing a function on a continuous domain using a sequence of numbers.\nFourier Transforms This section is for people who might have encountered the Fourier transform before, and want to see the Hilbert space interpretation of it.\nSuppose $f\\in L^2([0,1])$ is a signal, which means that $f(t)$ is the amplitude of the signal at time $t$. The sinusoid is everybodyâ€™s favorite example of a signal, given by $f(t)=\\sin (t)$. An orthogonal basis for $L^2([0, 1])$ is $(e_k)_{k\\in \\mathbb Z}$, where\n\\[e_k(t)=\\frac{1}{\\sqrt{2 \\pi}}e^{2 \\pi i k{t}}\\]\nHere, $\\mathbb Z$ are the integers, which is still a countable set because we can count them as $(0, 1, -1, 2, -2, \\dots)$. In this case, our isomorphism $T$ from $L^2([0,1])$ to $\\ell^2$ is given by:\n\\[T(f) = \\left(\\langle f, e_k \\rangle \\right)_{k\\in \\mathbb Z}\\] \\[T(f) = \\left( \\frac{1}{\\sqrt{2\\pi}}\\int_{0}^1 f(t)e^{-2\\pi i kt} dt\\right)_{k\\in \\mathbb Z}\\] These are exactly the Fourier coefficients (up to a constant factor, depending on how you define them)! Each coefficient tells you how much of a certain frequency is present in a signal. The way we have defined them (with proper normalization of the basis vectors) ensures that the Fourier transform $T$ is an isometric isomorphism. In fact, the observation that\n\\[\\langle f, g\\rangle_{L^2([0,1])} = \\langle T(f), T(g)\\rangle_{\\ell^2}\\] has a special name in the signal processing community: itâ€™s called Parsevalâ€™s theorem. We can also truncate the small Fourier coefficients to compress (or de-noise) a signal by ignoring its weak (or bothersome) frequencies â€“ be it an audio signal or an image. This truncation is a special case of the projection in Hilbert spaces, so itâ€™s in fact the best approximation in terms of the $L^2$ norm of the approximation error.\nOne of the motivations for defining the map $T$ is that we can now represent objects in an arbitrary Hilbert space using a (countable) set of numbers (in $\\ell^2$). Aside from being a powerful theoretical tool, it lets us store and manipulate these objects on computers efficiently, as evidenced by the example of the Fourier transform.\nAll that said, the reason I love typing out posts like these is because itâ€™s so gratifying to see all of these different mathematical objects come together under a unifying idea. The interplay between vectors, sequences, and functions is something that was never taught or emphasized to me in school. All throughout college, my instructors usually pulled the Fourier transform out of their hat, just to use it for 2 lectures and then put it back in before I ever figured out what it was.\n",
  "wordCount" : "1847",
  "inLanguage": "en",
  "datePublished": "2023-04-21T12:09:09-04:00",
  "dateModified": "2023-04-21T12:09:09-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shirazkn.github.io/posts/hilbert-spaces/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shiraz",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shirazkn.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shirazkn.github.io" accesskey="h" title="Shiraz (Alt + H)">Shiraz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shirazkn.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://shirazkn.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title">
      Hilbert Spaces
    </h1>
    <div class="post-meta"><span title='2023-04-21 12:09:09 -0400 EDT'>April 21, 2023</span>

</div>
  </header> 
  <div class="post-content"><!-- 
Consider the optimization problem

<p>
\[
\underset{x\in \mathcal C}{\text{minimize}}\quad f(x)
\]
</p>

where $\mathcal C\subseteq \mathcal X$ is called the *feasible* or the *constraint set*. Here, 

In engineering applications, $f(x)$ corresponds to an error term, or the distance between two objects in a Hilbert space, and so on. It can also be the negative of something we want to $\text{maximize}$.  -->
<p>Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is <span class=accented>complete</span>, i.e., it doesn&rsquo;t have er&hellip; holes in it. <a href="/posts/norms_metrics" class="accented">
    Recall
</a> that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors. Mathematicians sometimes use &lsquo;Hilbert space&rsquo; to refer specifically to infinite-dimensional inner product spaces, but for our purposes, we will conflate the usage of &lsquo;Hilbert space&rsquo; to include the finite-dimensional case.</p>
<p>Some interesting Hilbert spaces are given below. Try to think about what the induced norm $\lVert x \rVert_{\mathcal X}=\sqrt{\langle x, x\rangle}_{\mathcal X}$ represents in each of these spaces:</p>
<table>
<thead>
<tr>
<th>Hilbert Space $\mathcal X$</th>
<th>Inner Product $\langle \cdot, \cdot\rangle _\mathcal X$</th>
</tr>
</thead>
<tbody>
<tr>
<td>Numbers $a$, $b\in \mathbb R$</td>
<td>$ab$</td>
</tr>
<tr>
<td>Random Variables $X,Y\in \mathbb R$</td>
<td>$\mathbb E \left[XY \right]$</td>
</tr>
<tr>
<td>Real Vectors $x$, $y\in \mathbb R^n$</td>
<td>$x^\intercal y$</td>
</tr>
<tr>
<td>Complex Vectors $x$, $y\in \mathbb C^n$</td>
<td>$x^\dagger y$</td>
</tr>
<tr>
<td>Matrices $A$, $B\in \mathbb R^{m\times n}$</td>
<td>$\ \text{Trace}(A^{\intercal}B)$</td>
</tr>
<tr>
<td>Sequences $(x_i)_{i=1}^{\infty}, (y_i)_{i=1}^{\infty}\in \ell ^2(\mathbb R) \ $</td>
<td>$\sum_{i=1}^{\infty} x_i y_i$</td>
</tr>
<tr>
<td>Square-Integrable Functions $f,g \in L^2(\mathbb R)$</td>
<td>$\int_{-\infty}^{\infty}f(x)\overline{g(x)}dx$</td>
</tr>
</tbody>
</table>
<p><sub><sub>
Here, $\overline{({}\cdot{})}$ is the conjugate of a complex number (where you replace $i$ with $-i$), $({}\cdot{})^\dagger =\overline{({}\cdot{})}^\intercal $ is called the conjugate transpose of a complex vector.</sub></sub></p>
<h3 id="projections">Projections<a hidden class="anchor" aria-hidden="true" href="#projections">#</a></h3>
<p>A defining feature of Hilbert Spaces is their geometry. Because they have <a href="/posts/norms_metrics" class="accented">
    notions of angles
</a>, we have</p>
<p>
\[0 \leq {|\langle x, y\rangle_{\mathcal X} |} \leq {\|x\|\|y\|}\]
</p>
<p>where if the first equality holds we say $x$ and $y$ are <em>orthogonal</em> to each other, and the second equality holds if and only if they are linearly dependent: $x = a y$ for $a \in \mathbb R$.</p>
<p>We can define the <em>projection</em> of an element $x\in \mathcal X$ onto a subspace $S\subseteq \mathcal X$:</p>
<p>
\[ \text{P}_S(x) = \argmin_{y\in S}\|x-y\|_{\mathcal X}\]
</p>
<p>where $\lVert v \rVert_{\mathcal X} = \sqrt{\langle v, v\rangle}_{\mathcal X}$ is the induced norm.
The remarkable by-product of these definitions of angles, orthogonality, and projection, is that it is consistent with our Euclidean intuition: $x-\text{P}_S(x)$ is always orthogonal to $S$. A <a href="https://en.wikipedia.org/wiki/Hilbert_projection_theorem" target="_blank" class="accented">
    related theorem
</a> says that $\text P_{\bar S}(x)$ is well-defined and unique even if $\tilde S$ is any closed convex set in $\mathcal X$, although in this case we do not have orthogonality of $x-\text{P}_{\tilde S}(x)$ to the other elements in $\tilde S$.</p>
<div>
<!-- <figure class=invertible style="max-width: 25%;"> -->
<figure class=invertible style="max-width: 100%;">
<img src=/post-images/linear_algebra/projection.png>
</figure>
</div>
<p>As an example, in the Hilbert space of random variables, for its elements $X$ and $Y$, orthogonality corresponds to $\mathbb E\left[X Y\right] =  0$, the norm becomes the variance, and (orthogonal) <a href="https://inst.eecs.berkeley.edu/~ee126/sp18/projection.pdf" target="_blank" class="accented">
    projection gives the least-squares estimator of given a random variable
</a>. More generally, projection can be used to <a href="https://math.stackexchange.com/questions/3000704/orthogonal-projection-on-a-polynomial-space" target="_blank" class="accented">
    best approximate
</a> an element of a Hilbert space using a finite set of basis vectors of that (or another) Hilbert space.</p>
<hr> 
<h3 id="the-mathbb-rn-lvertcdotrvert_2-space-of-sequences">The $(\mathbb R^N, \lVert{}\cdot{}\rVert_2)$ Space of Sequences<a hidden class="anchor" aria-hidden="true" href="#the-mathbb-rn-lvertcdotrvert_2-space-of-sequences">#</a></h3>
<p>This looks just like the Euclidean space of $N$-dimensional vectors. I mean, a vector is a set of $N$ coefficients (real numbers) where each coefficient describes the vector&rsquo;s distance along the corresponding standard basis vector (or &lsquo;axis&rsquo;, if you will).</p>
<!-- Instead of axis, we should say orthonormal basis. -->
<p>Now, let&rsquo;s ignore all of that and our geometric intuition of vectors, and consider that $\mathbb R^N = \mathbb R \times \mathbb  R \times \dots \times \mathbb R$ is just a set-theoretic definition. An element $x$ of this space is a sequence of real numbers,</p>
<p>
\[x = (x_1, x_2, \dots, x_N)\]
</p>
<p>and the norm can be defined as $\left(\sum_i^N x_i^2\right)^{1/2}$. So, nowhere did we have to talk about vectors or matrices. We can generalize this to $N\rightarrow \infty$.</p>
<h3 id="the-ell2-space-of-sequences">The $\ell^2$ Space of Sequences<a hidden class="anchor" aria-hidden="true" href="#the-ell2-space-of-sequences">#</a></h3>
<p>$\ell^2(\mathbb R)$ consists of countable sequences of real numbers. &lsquo;Countable&rsquo; here means that we can count them like we can count the natural numbers, but they are infinitely long nonetheless. We denote this sequence as $(x_i)_{i=1}^{\infty}$.
A sequence is in $\ell^2$ if and only if it is <span class=accented>square-summable</span>, which means</p>
<p>\[
    \sum_{i=1}^{\infty}|x_i|^2 < \infty 
\]</p>
<p>A sequence which is not in $\ell^2$ (i.e., an &lsquo;infinite-length vector&rsquo;) is
$\left(\frac{1}{\sqrt{1}}, \frac{1}{\sqrt{2}}, \frac{1}{\sqrt{3}}, \dots\right)$ because its sum-of-squares decays too slowly as the number of terms increases. A sequence that is in $\ell ^2$ is $\left(\frac{1}{1}, \frac{1}{2}, \frac{1}{3}, \dots\right)$.</p>
<!-- [^aside]: $\ell^p$ spaces (with the $p$-norm defined analogously) can shed some insight into how vector spaces behave in the limit $N\rightarrow \infty$! Perhaps you could [revisit my post](/posts/balls) on norm balls, think of higher dimensional Euclidean spaces as tending towards the $\ell^2$ space. -->
<h3 id="separable-hilbert-spaces-are-isomorphic">(Separable) Hilbert Spaces are Isomorphic<a hidden class="anchor" aria-hidden="true" href="#separable-hilbert-spaces-are-isomorphic">#</a></h3>
<p><a href="/posts/cat_theory_1" class="accented">
    Isomorpshisms
</a> are maps from one type of mathematical object to another that preserve its structure.
<a href="http://mathonline.wikidot.com/separable-hilbert-spaces-are-isometrically-isomorphic-to-2" target="_blank" class="accented">
    All (separable) infinite-dimensional Hilbert spaces are isomorphic to the $\ell^2$ space
</a>, which is a fancy way of saying that we can do the following: We first construct a countable orthonormal basis for $\mathcal X$, denoted as $(e_i)_{i=1}^{\infty}$. Then, we define an isometric isomorphism (a distance-preserving, <a href="/posts/cat_theory_1" class="accented">
    structure-preserving
</a> mapping) from $\mathcal X$ to $\ell^2$, as follows:</p>
<p>
\[T: \mathcal X \rightarrow \ell^2\]
\[T(x) = (\langle e_i,x \rangle_{\mathcal X})_{i=1}^{\infty} \]
</p>
<p>which is the set of coefficients of $x$ along each basis vector. Note that $T$ is basis-dependent, we could pick a different basis and get a different $T$.
Since $T$ is an isomorphism (i.e., does not &lsquo;destroy&rsquo; information during the mapping), we may hope to be able to go back from $\ell^2$ to $\mathcal X$:</p>
<p>
\[T^*: \ell^2 \rightarrow \mathcal X\]
\[T^{*}\left((c_i)_{i=1}^{\infty}\right) = \sum_{i=1}^{\infty} c_i e_i \]
</p>
<p>Here, $T^{*}$ is called the <em>adjoint</em> of $T$; it is an operator that satisfies</p>
<p>\[\langle y,T(x)\rangle _{\ell^2} = \langle T^*(y),x\rangle _{\mathcal X}\]</p>
<p>It can be shown that the map $T({}\cdot{})$ <a href="https://proofwiki.org/wiki/Surjection_that_Preserves_Inner_Product_is_Linear" target="_blank" class="accented">
    is necessarily linear
</a>, and that <a href="https://proofwiki.org/wiki/Linear_Transformation_is_Isomorphism_iff_Inverse_Equals_Adjoint" target="_blank" class="accented">
    $T^*$ is also the inverse of $T$
</a>.
The discussion thus far should be reminding us of linear transformations in undergraduate linear algebra. Indeed, the mapping $T({}\cdot{})$ can always be represented via a matrix, although this matrix can be infinite dimensional in general.</p>
<p>In the finite-dimensional case, we can just use the Gram-Schmidt process to construct an orthonormal basis. Using a similar reasoning as above, we see that all (separable) Hilbert spaces with dimension $N$ are isomorphic to the $(\mathbb R^N, \lVert{}\cdot{}\rVert_2)$ space.
The condition that $T^*=T^{-1}$ might remind you of <a href="https://en.wikipedia.org/wiki/Unitary_matrix" target="_blank" class="accented">
    unitary matrices
</a> ($U^\dagger = U^{-1}$, where $U^\dagger$ is the conjugate transpose of $U$), which are exactly the distance-preserving, structure-preserving matrices in $\mathbb C^N$.</p>
<p>When we&rsquo;re working in $\mathbb R^N$, we replace &lsquo;unitary&rsquo; with &lsquo;orthonormal&rsquo;, and we have $Q^\intercal = Q^{-1}$ for an orthonormal matrix $Q$. They are an isometry (distance-preserving) because $\lVert Qx\rVert_2 = \lVert x \rVert_2$, and they are an isomorphism (structure-preserving) because $\langle Qx, Qy \rangle =\langle x, Q^\intercal Qy \rangle = \langle x, y\rangle$.</p>
<p>Thus, we always have a (non-unique) unitary matrix (or a <a href="https://en.wikipedia.org/wiki/Unitary_operator" target="_blank" class="accented">
    unitary operator
</a>, in the infinite-dimensional case) which will take us from one vector space to &lsquo;another&rsquo; (but actually, it takes us to a rotated version of the same space ðŸ™ƒ) in a distance-preserving, structure-preserving manner.</p>
<hr> 
<h3 id="the-lp-space-of-functions">The $L^p$ Space of Functions<a hidden class="anchor" aria-hidden="true" href="#the-lp-space-of-functions">#</a></h3>
<p>Let&rsquo;s move onwards to function spaces.
The space $L^1(\mathbb R)$ is the space of <span class=accented>absolutely integrable</span> functions:</p>
<p>
\[ \| f\|_{L^1} =\int_{\mathbb R} | f(x)| dx < \infty
\]</p>
<p>where $f:\mathbb R\rightarrow \mathbb C$ (so yeah, its codomain can be complex-valued). It is not a Hilbert space, because we have not yet defined an inner product for this space. The space $L^2(\mathbb R)$ has functions which are <span class=accented>square-integrable</span>:</p>
<p>
\[ \| f\|_{L^2}^2 =\int_{\mathbb R} | f(x)|^2dx < \infty
\]</p>
<p>which is a Hilbert space because it has the following inner product + norm combination:</p>
<p>
\[ \langle f, g \rangle_{L^2} = \int_{\mathbb R} f(x) \overline{g(x)} dx\]
\[ \| f\|_{L^2} = \sqrt{ \langle f, f \rangle_{L^2}}\]
</p>
<p>where $\overline {z}$ is the complex conjugate of $z\in \mathbb C$. Can we use the above as an inner product for $L^1$ as well? It turns out that we can&rsquo;t, because even if $f$ is in $L^1$, the integral of &lsquo;$f(x)f(x)$&rsquo; can be unbounded if $f$ is not also in $L^2$.</p>
<!-- Other $L^p$ spaces are defined analogously. -->
<!-- This would violate the Cauchy-Schwarz inequality for any valid inner product, $| \langle x, y\rangle| \leq \lVert x \rVert \lVert y \rVert$. -->
<aside class=aside-center>
Observe that we always require some sort of 'boundedness of norm' when defining Hilbert spaces... it is related to the requirement of <i>completeness</i> of the space, which we so conveniently glossed over. 
</aside>
<p>Observe that the norms on $\ell^p$ and $L^p$ spaces are natural extensions of the <a href="/posts/norms_metrics" class="accented">
    $p$-norms
</a> for finite-dimensional vector spaces, and as expected, we have an inner product only when $p=2$. The $L^p$ space can be thought of as a &lsquo;refinement&rsquo; of the domain of the $\ell^p$ space, where the index set $\lbrace 1, 2, \dots, \infty\rbrace$ (which was countable) of $\ell^p$ is replaced with $\mathbb R$ (which is uncountable) in $L^p$. So even though these might seem like completely different concepts thrown together, they are <a href="https://en.wikipedia.org/wiki/Lp_space#Special_cases" target="_blank" class="accented">
    closely related
</a> and inherit much of each other&rsquo;s properties!</p>
<div>
<!-- <figure class=invertible style="max-width: 25%;"> -->
<figure class=invertible style="max-width: 100%;">
<img src=/post-images/linear_algebra/hilbert_spaces.png>
</figure>
</div>
<p>But since $L^2$ and $\ell^2$ are supposed to be isomorphic, this suggests that we could go from $L^2$ to $\ell^2$ using a linear map, which might seem bizarre at first &ndash; we would be representing a function on a continuous domain using a sequence of numbers.</p>
<h4 id="fourier-transformshttpsenmwikipediaorgwikihilbert_spacefourier_analysis"><a href="https://en.m.wikipedia.org/wiki/Hilbert_space#Fourier_analysis" target="_blank" class="accented">
    Fourier Transforms
</a></h4>
<p>This section is for people who might have encountered the Fourier transform before, and want to see the Hilbert space interpretation of it.</p>
<p>Suppose $f\in L^2([0,1])$ is a <em>signal</em>, which means that $f(t)$ is the amplitude of the signal at time $t$. The sinusoid is everybody&rsquo;s favorite example of a signal, given by $f(t)=\sin
(t)$.
An orthogonal basis for $L^2([0, 1])$ is $(e_k)_{k\in \mathbb Z}$, where</p>
<p>\[e_k(t)=\frac{1}{\sqrt{2 \pi}}e^{2 \pi i k{t}}\]</p>
<p>Here, $\mathbb Z$ are the integers, which is still a countable set because we can count them as $(0, 1, -1, 2, -2, \dots)$.
In this case, our isomorphism $T$ from $L^2([0,1])$ to $\ell^2$ is given by:</p>
<p>
\[T(f) = \left(\langle f, e_k \rangle \right)_{k\in \mathbb Z}\]
</p>
<p>
\[T(f) = \left( \frac{1}{\sqrt{2\pi}}\int_{0}^1 f(t)e^{-2\pi i kt} dt\right)_{k\in \mathbb Z}\]
</p>
<p>These are exactly the Fourier coefficients (up to a constant factor, depending on how you define them)! Each coefficient tells you how much of a certain frequency is present in a signal. The way we have defined them (with proper normalization of the basis vectors) ensures that the Fourier transform $T$ is an isometric isomorphism. In fact, the observation that</p>
<p>\[\langle f, g\rangle_{L^2([0,1])} = \langle T(f), T(g)\rangle_{\ell^2}\]
</p>
<p>has a special name in the signal processing community: it&rsquo;s called <a href="https://en.wikipedia.org/wiki/Parseval's_theorem" class=accented>Parseval&rsquo;s theorem</a>. We can also truncate the small Fourier coefficients to <em><a href="https://www.dspguide.com/ch27/6.htm" target="_blank" class="accented">
    compress
</a></em> (or <em>de-noise</em>) a signal by ignoring its weak (or bothersome) frequencies &ndash; be it an audio signal or an image. This truncation is a special case of the projection in Hilbert spaces, so it&rsquo;s in fact the best approximation in terms of the $L^2$ norm of the approximation error.</p>
<hr> 
<p>One of the motivations for defining the map $T$ is that we can now represent objects in an arbitrary Hilbert space using a (countable) set of numbers (in $\ell^2$). Aside from being a powerful theoretical tool, it lets us store and manipulate these objects on computers efficiently, as evidenced by the example of the Fourier transform.</p>
<p>All that said, the reason I love typing out posts like these is because it&rsquo;s so gratifying to see all of these different mathematical objects come together under a unifying idea. The interplay between vectors, sequences, and functions is something that was never taught or emphasized to me in school. All throughout college, my instructors usually pulled the Fourier transform out of their hat, just to use it for 2 lectures and then put it back in before I ever figured out what it was.</p>
<!-- Aha! I'm now beginning to understand some of the things that were kept hidden from me... -->
<!-- https://math.stackexchange.com/questions/2739175/do-we-really-need-to-specify-a-basis-to-describe-a-tuple -->

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shirazkn.github.io/tags/linear-algebra/">Linear Algebra</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://shirazkn.github.io">Shiraz</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><div class="headerfooter">
    <sub><sup><sub>&#9786;</sub></sup></sub>
</div>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
