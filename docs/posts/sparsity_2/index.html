<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Understanding Sparsity through Sub-Gradients | Shiraz</title>
<meta name="keywords" content="Optimization">
<meta name="description" content="

    We talked about
 why sparsity plays an important role in many of the 
    inverse problems
 that we encounter in engineering. To actually find the sparse solutions to these problems, we add &lsquo;sparsity-promoting&rsquo; terms to our optimization problems; the machine learning community calls this approach regularization.">
<meta name="author" content="">
<link rel="canonical" href="https://shirazkn.github.io/posts/sparsity_2/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.f63ca8b26bc4bdec5a73358cc90732c3c03880bb9f674e5868ae860e17b5d924.css" integrity="sha256-9jyosmvEvexaczWMyQcyw8A4gLufZ05YaK6GDhe12SQ=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://shirazkn.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://shirazkn.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://shirazkn.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://shirazkn.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://shirazkn.github.io/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://shirazkn.github.io/posts/sparsity_2/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>


<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      packages: {'[+]': ['mathtools']},
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]                  
    },
    loader:{
      load: ['ui/safe', '[tex]/mathtools']
    },
  };
</script>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IM+Fell+DW+Pica"><meta property="og:url" content="https://shirazkn.github.io/posts/sparsity_2/">
  <meta property="og:site_name" content="Shiraz">
  <meta property="og:title" content="Understanding Sparsity through Sub-Gradients">
  <meta property="og:description" content=" We talked about why sparsity plays an important role in many of the inverse problems that we encounter in engineering. To actually find the sparse solutions to these problems, we add ‚Äòsparsity-promoting‚Äô terms to our optimization problems; the machine learning community calls this approach regularization.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2023-04-28T17:15:26-04:00">
    <meta property="article:modified_time" content="2023-04-28T17:15:26-04:00">
    <meta property="article:tag" content="Optimization">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Understanding Sparsity through Sub-Gradients">
<meta name="twitter:description" content="

    We talked about
 why sparsity plays an important role in many of the 
    inverse problems
 that we encounter in engineering. To actually find the sparse solutions to these problems, we add &lsquo;sparsity-promoting&rsquo; terms to our optimization problems; the machine learning community calls this approach regularization.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://shirazkn.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Understanding Sparsity through Sub-Gradients",
      "item": "https://shirazkn.github.io/posts/sparsity_2/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding Sparsity through Sub-Gradients",
  "name": "Understanding Sparsity through Sub-Gradients",
  "description": " We talked about why sparsity plays an important role in many of the inverse problems that we encounter in engineering. To actually find the sparse solutions to these problems, we add \u0026lsquo;sparsity-promoting\u0026rsquo; terms to our optimization problems; the machine learning community calls this approach regularization.\n",
  "keywords": [
    "Optimization"
  ],
  "articleBody": " We talked about why sparsity plays an important role in many of the inverse problems that we encounter in engineering. To actually find the sparse solutions to these problems, we add ‚Äòsparsity-promoting‚Äô terms to our optimization problems; the machine learning community calls this approach regularization.\nRegularization An optimization method that was popularized in the $80$s and $90$s is the LASSO , also called $L^1$ norm regularization, which solves problems of the following form:\n\\[ \\begin{array}{ll} \\underset{x\\in \\mathbb R^n}{\\textrm{minimize}} \u0026 g(x) + \\lambda \\|x\\|_1 \\end{array} \\] Usually, $g(x)$ corresponds to an error/loss term. It can also be the negative of something we wish to $\\text{maximize}$. The claim is that the additional term $\\lVert x\\rVert_1$ promotes the sparsity of the solution $x^\\star$, i.e., it attempts to set one or more elements of $x^\\star$ to $0$. Similarly, in ridge regression (also called Tikhonov regularization or $L^2$ norm regularization), we add a $\\lVert x\\rVert_2^2$ term to the objective. This is known to shrink the solution towards the origin, but it does not necessarily make the solution sparse.1\nWhat about $\\lVert x\\rVert_2$ (as opposed to $\\lVert x\\rVert_2^2$), what would that do? How do we reason about an arbitrary ‚Äòregularization term‚Äô and interpret what it does? If you have encountered this question before, then you‚Äôve likely seen explanations such as this one . üëàüèΩ While that‚Äôs a great, conversational explainer on sparsity, I want to give it a slightly more formal treatment for anyone interested.\nSub-Gradient Descent I expect that the reader is familiar with gradient descent and convex functions. I will offer a brief introduction to sub-gradient descent, which extends gradient descent to the case where the objective function is non-differentiable, but still convex.\nA non-differentiable function is one that does not have a well-defined gradient at one or more points of its domain. But if the function is convex (i.e., bowl-shaped), then it has the next best thing: a sub-gradient of $f(x)$ at $x^\\star$ is a vector $w$, such that the inequality\n\\[ f(x) - f(x^\\star) \\geq w^\\intercal (x-x^\\star)\\] holds for all $x$ in the domain of $f$. The sub-gradient $w$ is not unique in general. However, if $f$ is differentiable at $x^\\star$, then $w$ takes the unique value of $\\nabla f(x^\\star)$. A convex function has at least one sub-gradient at every point of its domain; we can prove that fact using this theorem . Observe that sub-gradients can be thought of as hyperplanes that touch or support the function from below, similar to how the gradient of a differentiable convex function touches it from below.\nSince the sub-gradient is non-unique, we define the sub-differential of $f$ at $x^\\star$, denoted as $\\partial f(x^\\star)$, as the set of all sub-gradients of $f$ at $x^\\star$. We can now do gradient descent, but instead of the gradient, we pick a sub-gradient direction to descend towards. This procedure of sub-gradient descent is motivated by the following fact: $x^\\star$ is the global minimizer of $f(x)$ if and only if $\\mathbf 0 \\in \\partial f(x^\\star)$. For differentiable functions, sub-gradient descent reduces to gradient descent.\nSimilar to how, for differentiable functions,\n\\[\\nabla(f + g)(x)=\\nabla f(x) + \\nabla g(x),\\]\nwe have\n\\[\\partial (f+g)(x) = \\partial f (x) + \\partial g(x) \\] However, we are dealing with sets and not vectors in the non-differentiable case. The ‚Äò$+$‚Äô in the preceding equation refers to the Minkowski sum; for sets $\\mathcal A$ and $\\mathcal B$,\n\\[ \\mathcal A + \\mathcal B = \\left\\lbrace a+b | a\\in \\mathcal A, b \\in \\mathcal B \\right\\rbrace\\] Revisiting the LASSO With this, let‚Äôs look at the LASSO-type problem,\n\\[\\begin{array}{ll} \\underset{x\\in \\mathbb R^n}{\\textrm{minimize}} \u0026 f(x) + g(x) \\end{array} \\] where the green lines show the sub-gradients of the two functions at $x^\\star$. This function is minimized whenever we can pick sub-gradients from $f$ and $g$ such that they ‚Äòcancel each other out‚Äô.\n\\[f+g\\text{ is minimized at }x^\\star\\\\ \\Updownarrow \\\\ \\mathbf 0 \\in \\partial (f+g)(x^\\star) \\\\ \\Updownarrow\\\\ \\exists w \\in \\partial f (x^\\star) \\text{ \\ such that \\ } w+ \\nabla g(x^\\star) =\\mathbf 0\\] At the differentiable points ($x\\neq x^\\star$), neither function has much freedom in picking a sub-gradient. But at $x^\\star$, $f(x)$ has a range of sub-gradients to pick from; it can choose one that ‚Äòcancels out‚Äô the corresponding (sub-)gradient of $g(x)$ at $x^\\star$. This is why a convex, non-differentiable regularization term is likely to pull the solution towards its non-differentiable points!\nChoosing a ‚ÄòRegularization Term‚Äô Suppose $x\\in \\mathbb R^2$. The function $\\lVert x \\rVert_1$ has the following shape:\nwhere the green plane is a sub-gradient at the origin. Since $\\lVert x \\rVert_1$ is non-differentiable along the axes, it tries to snap the minima towards the axes. Note that the axes of $\\mathbb R^2$ are exactly where the sparse vectors are. What about when $x\\in \\mathbb R^3$? At what points is $\\lVert x\\rVert_1$ non-differentiable then? (Hint: it‚Äôs not just the axes!)\nThe function $\\lVert x \\rVert_2$ looks like an ice-cream cone:\nsince it‚Äôs only non-differentiable at the origin, it tries to snap the solution towards the origin. This is different from ridge regression, which instead uses $\\lVert x\\rVert_2 ^2$. The function $\\lVert x\\rVert_2 ^2$ is differentiable everywhere; it is ‚Äòbowl-shaped‚Äô. It pulls the solution towards the origin, but does not particularly demand that the solution be exactly $\\mathbf 0$. So is there a use for $\\lVert x \\rVert_2$? Yes! It can be used to promote the block-sparsity of a vector, where the $0$‚Äôs of the vector appear in blocks. Consider\n\\[ x^\\intercal = \\left[\\ x_1^\\intercal\\ x_2^\\intercal\\ x_3^\\intercal \\dots x_n^\\intercal\\ \\right] \\] where $x_i \\in \\mathbb R^{d_i}$, and $x \\in \\mathbb R^{\\sum_{i=1}^n d_i}$. Suppose we know that the sparsity of $x$ occurs in blocks, i.e., some of the $x_i$ are full of zeros. Then, the regularization term $\\sum_{i=1}^{n}\\lVert x_i \\rVert_2$ is what we want to use since it sets some of the $x_i$ to $\\mathbf 0$ but does not promote sparsity within each block. (I used this fact to solve an engineering problem in my PhD dissertation .)\nClosing Note There are many different ways to think about sparsity. For instance, one could imagine trying to balance a tennis ball that is resting on one of the surfaces we showed above, by holding the surface from below and tilting it. The ball is likely to settle at one of the non-differentiable points of the surface, thereby minimizing its potential energy. I like the sub-gradient interpretation because it works irrespective of the dimension. We can test for differentiability of arbitrary functions even if we cannot visualize them.\nAs an aside, LASSO and ridge regression can be studied using the theory of proximal operators .¬†‚Ü©Ô∏é\n",
  "wordCount" : "1092",
  "inLanguage": "en",
  "datePublished": "2023-04-28T17:15:26-04:00",
  "dateModified": "2023-04-28T17:15:26-04:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shirazkn.github.io/posts/sparsity_2/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shiraz",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shirazkn.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://shirazkn.github.io/" accesskey="h" title="Shiraz (Alt + H)">Shiraz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://shirazkn.github.io/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="https://shirazkn.github.io/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Understanding Sparsity through Sub-Gradients
    </h1>
    <div class="post-meta"><span title='2023-04-28 17:15:26 -0400 EDT'>April 28, 2023</span>

</div>
  </header> 
  <div class="post-content"><!-- This post will require some familiarity with optimization (or least-squares, if you will). -->
<p><a href="/posts/sparsity" class="accented">
    We talked about
</a> why sparsity plays an important role in many of the <a href="https://en.wikipedia.org/wiki/Inverse_problem" target="_blank" class="accented">
    inverse problems
</a> that we encounter in engineering. To actually find the sparse solutions to these problems, we add &lsquo;sparsity-promoting&rsquo; terms to our optimization problems; the machine learning community calls this approach <em>regularization</em>.</p>
<h3 id="regularization">Regularization<a hidden class="anchor" aria-hidden="true" href="#regularization">#</a></h3>
<p>An optimization method that was popularized in the $80$s and $90$s is the <a href="https://en.wikipedia.org/wiki/Lasso_%28statistics%29" target="_blank" class="accented">
    LASSO
</a>, also called $L^1$ norm regularization, which solves problems of the following form:</p>
<p>
\[
    \begin{array}{ll}
\underset{x\in \mathbb R^n}{\textrm{minimize}} & g(x) + \lambda \|x\|_1
\end{array}
    \]
</p>
<p>Usually, $g(x)$ corresponds to an error/loss term. It can also be the negative of something we wish to $\text{maximize}$.
The claim is that the additional term $\lVert x\rVert_1$ promotes the sparsity of the solution $x^\star$, i.e., it attempts to set one or more elements of $x^\star$ to $0$. Similarly, in <a href="https://en.wikipedia.org/wiki/Ridge_regression" target="_blank" class="accented">
    ridge regression
</a> (also called Tikhonov regularization or $L^2$ norm regularization), we add a $\lVert x\rVert_2^2$ term to the objective. This is known to shrink the solution towards the origin, but it does not necessarily make the solution sparse.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>What about $\lVert x\rVert_2$ (as opposed to $\lVert x\rVert_2^2$), what would that do? How do we reason about an arbitrary &lsquo;regularization term&rsquo; and interpret what it does?
If you have encountered this question before, then you&rsquo;ve likely seen explanations <a href="https://blog.mlreview.com/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a" target="_blank" class="accented">
    such as this one
</a>. üëàüèΩ While that&rsquo;s a great, conversational explainer on sparsity, I want to give it a slightly more formal treatment for anyone interested.</p>
<h3 id="sub-gradient-descent">Sub-Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#sub-gradient-descent">#</a></h3>
<p>I expect that the reader is familiar with gradient descent and convex functions. I will offer a brief introduction to sub-gradient descent, which extends gradient descent to the case where the objective function is non-differentiable, but still convex.</p>
<p>A non-differentiable function is one that does not have a well-defined gradient at one or more points of its domain. But if the function is <em>convex</em> (i.e., bowl-shaped), then it has the next best thing:
a <span class=accented>sub-gradient</span> of $f(x)$ at $x^\star$ is a vector $w$, such that the inequality</p>
<p>
\[ f(x) - f(x^\star) \geq w^\intercal (x-x^\star)\]
</p>
<p>holds for all $x$ in the domain of $f$. The sub-gradient $w$ is not unique in general. However, if $f$ is differentiable at $x^\star$, then $w$ takes the unique value of $\nabla f(x^\star)$. A convex function has at least one sub-gradient at every point of its domain; we can prove that fact using <a href="https://en.wikipedia.org/wiki/Supporting_hyperplane" target="_blank" class="accented">
    this theorem
</a>. Observe that sub-gradients can be thought of as hyperplanes that <em>touch</em> or <em>support</em> the function from below, similar to how the gradient of a differentiable convex function touches it from below.</p>
<p>Since the sub-gradient is non-unique, we define the <span class=accented>sub-differential</span> of $f$ at $x^\star$, denoted as $\partial f(x^\star)$, as the set of all sub-gradients of $f$ at $x^\star$. We can now do gradient descent, but instead of <em>the</em> gradient, we pick <em>a</em> sub-gradient direction to descend towards. This procedure of <span class=accented>sub-gradient descent</span> is motivated by the following fact: $x^\star$ is the global minimizer of $f(x)$ if and only if $\mathbf 0 \in \partial f(x^\star)$.
For differentiable functions, sub-gradient descent reduces to gradient descent.</p>
<p>Similar to how, for differentiable functions,</p>
<p>\[\nabla(f + g)(x)=\nabla f(x) + \nabla g(x),\]</p>
<p>we have</p>
<p>
\[\partial (f+g)(x) = \partial f (x) + \partial g(x) \]
</p>
<p>However, we are dealing with sets and not vectors in the non-differentiable case. The &lsquo;$+$&rsquo; in the preceding equation refers to the <em>Minkowski sum</em>; for sets $\mathcal A$ and $\mathcal B$,</p>
<p>
\[ \mathcal A + \mathcal B  = \left\lbrace a+b | a\in \mathcal A, b \in \mathcal B \right\rbrace\]
</p>
<h4 id="revisiting-the-lasso">Revisiting the LASSO<a hidden class="anchor" aria-hidden="true" href="#revisiting-the-lasso">#</a></h4>
<p>With this, let&rsquo;s look at the LASSO-type problem,</p>
<p>
\[\begin{array}{ll}
\underset{x\in \mathbb R^n}{\textrm{minimize}} & f(x) + g(x)
\end{array}
    \]
</p>
<div>
<figure class=invertible style="max-width: 100%;">
<img src=/post-images/optimization/lasso_competing.png>
</figure>
</div>
<p>where the green lines show the sub-gradients of the two functions at $x^\star$. This function is minimized whenever we can pick sub-gradients from $f$ and $g$ such that they &lsquo;cancel each other out&rsquo;.</p>
<div>
<figure class=invertible style="max-width: 82%;">
<img src=/post-images/optimization/lasso_combined.png>
</figure>
</div>
<p>
\[f+g\text{ is minimized at }x^\star\\
\Updownarrow
\\
\mathbf 0 \in \partial (f+g)(x^\star) \\
\Updownarrow\\
\exists w \in \partial f (x^\star) \text{ \ such that \ } w+ \nabla g(x^\star) =\mathbf 0\]
</p>
<p>At the differentiable points ($x\neq x^\star$), neither function has much freedom in picking a sub-gradient. But at $x^\star$, $f(x)$ has a range of sub-gradients to pick from; it can choose one that &lsquo;cancels out&rsquo; the corresponding (sub-)gradient of $g(x)$ at $x^\star$. This is why a convex, non-differentiable regularization term is likely to pull the solution towards its non-differentiable points!</p>
<h3 id="choosing-a-regularization-term">Choosing a &lsquo;Regularization Term&rsquo;<a hidden class="anchor" aria-hidden="true" href="#choosing-a-regularization-term">#</a></h3>
<p>Suppose $x\in \mathbb R^2$. The function <span class=accented>$\lVert x \rVert_1$</span> has the following shape:</p>
<div>
<figure class=invertible style="max-width: 95%;">
<img src=/post-images/optimization/L1Norm.png>
</figure>
</div>
<p>where the green plane is a sub-gradient at the origin.
Since $\lVert x \rVert_1$ is non-differentiable along the axes, it tries to <em>snap</em> the minima towards the axes. Note that the axes of $\mathbb R^2$ are exactly where the sparse vectors are.
What about when $x\in \mathbb R^3$? At what points is $\lVert x\rVert_1$ non-differentiable then? (Hint: it&rsquo;s not just the axes!)</p>
<!-- $\lVert x \rVert_1$ always promotes sparsity, even in higher dimensions. -->
<p>The function <span class=accented>$\lVert x \rVert_2$</span> looks like an ice-cream cone:</p>
<div>
<figure class=invertible style="max-width: 95%;">
<img src=/post-images/optimization/L2Norm.png>
</figure>
</div>
<p>since it&rsquo;s only non-differentiable at the origin, it tries to snap the solution towards the origin. This is different from ridge regression, which instead uses <span class=accented>$\lVert x\rVert_2 ^2$</span>. The function $\lVert x\rVert_2 ^2$ is differentiable everywhere; it is &lsquo;bowl-shaped&rsquo;. It pulls the solution towards the origin, but does not particularly demand that the solution be exactly $\mathbf 0$. So is there a use for $\lVert x \rVert_2$? Yes! It can be used to promote the <a href="https://www.sciencedirect.com/science/article/pii/S0165168417300683" target="_blank" class="accented">
    block-sparsity
</a> of a vector, where the $0$&rsquo;s of the vector appear in blocks. Consider</p>
<p>
\[ x^\intercal = \left[\ x_1^\intercal\ x_2^\intercal\ x_3^\intercal \dots x_n^\intercal\ \right] \]
</p>
<p>where $x_i \in \mathbb R^{d_i}$, and $x \in \mathbb R^{\sum_{i=1}^n d_i}$. Suppose we know that the sparsity of $x$ occurs in blocks, i.e., some of the $x_i$ are full of zeros.
Then, the regularization term $\sum_{i=1}^{n}\lVert x_i \rVert_2$ is what we want to use since it sets some of the $x_i$ to $\mathbf 0$ but does not promote sparsity <em>within</em> each block. (I used this fact to solve an engineering problem in my <a href="https://hammer.purdue.edu/articles/thesis/Robustness_Resilience_and_Scalability_of_State_Estimation_Algorithms/24658653" target="_blank" class="accented">
    PhD dissertation
</a>.)</p>
<h4 id="closing-note">Closing Note<a hidden class="anchor" aria-hidden="true" href="#closing-note">#</a></h4>
<p>There are many different ways to think about sparsity. For instance, one could imagine trying to balance a tennis ball that is resting on one of the surfaces we showed above, by holding the surface from below and tilting it. The ball is likely to settle at one of the non-differentiable points of the surface, thereby minimizing its potential energy. I like the sub-gradient interpretation because it works irrespective of the dimension. We can test for differentiability of arbitrary functions even if we cannot visualize them.</p>
<!-- Moreover, the sub-gradient interpretation speaks of the global minima of the function, whereas the 'tennis ball' analogy only says that the ball will rest at one of the local minima. --><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>As an aside, LASSO and ridge regression can be studied using the theory of <a href="https://en.wikipedia.org/wiki/Proximal_operator" target="_blank" class="accented">
    proximal operators
</a>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://shirazkn.github.io/tags/optimization/">Optimization</a></li>
    </ul>
  </footer>
<script src="https://giscus.app/client.js"
        data-repo="shirazkn/shirazkn.github.io"
        data-repo-id="R_kgDOI2VbWw"
        data-category="Announcements"
        data-category-id="DIC_kwDOI2VbW84CWJnt"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="noborder_light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    
    <div class="signup-form">
        <script async src="https://eocampaign1.com/form/e5dbf9e6-b891-11ee-a1b7-cdaf9e8a98be.js" data-form="e5dbf9e6-b891-11ee-a1b7-cdaf9e8a98be"></script>
    </div>
    
    
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><div class="headerfooter">
    <sub><sup><sub>&#9786;</sub></sup></sub>
</div>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
