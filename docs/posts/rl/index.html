<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>A Pedantic Introduction to VLA Models | Shiraz</title>
<meta name="keywords" content="Mechanics, Optimization, Probability">
<meta name="description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">
<meta name="author" content="">
<link rel="canonical" href="http://10.0.0.26:1313/posts/rl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.05929664bad1843e2a6ecdf2027ade7534ece873c5d97efdded31397c1d46881.css" integrity="sha256-BZKWZLrRhD4qbs3yAnredTTs6HPF2X793tMTl8HUaIE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://10.0.0.26:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://10.0.0.26:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://10.0.0.26:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://10.0.0.26:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://10.0.0.26:1313/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://10.0.0.26:1313/posts/rl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      packages: {'[+]': ['mathtools']},
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    },
    loader:{
      load: ['ui/safe', '[tex]/mathtools']
    },
  };
</script>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IM+Fell+DW+Pica">

<meta property="og:url" content="http://10.0.0.26:1313/posts/rl/">
  <meta property="og:site_name" content="Shiraz">
  <meta property="og:title" content="A Pedantic Introduction to VLA Models">
  <meta property="og:description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:modified_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:tag" content="Mechanics">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Probability">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Pedantic Introduction to VLA Models">
<meta name="twitter:description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://10.0.0.26:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "A Pedantic Introduction to VLA Models",
      "item": "http://10.0.0.26:1313/posts/rl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "A Pedantic Introduction to VLA Models",
  "name": "A Pedantic Introduction to VLA Models",
  "description": "Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.",
  "keywords": [
    "Mechanics", "Optimization", "Probability"
  ],
  "articleBody": "␥ States \u0026 Observations Let $\\mathscr S$ be the state space of a robot, and $\\mathscr O$ the observation space. The state $S_t$ might include robot’s joint angles as measured/estimated during times $\\lbrace t-\\mathrm k \\varepsilon:\\mathrm k=0,1,\\cdots,\\mathrm n \\rbrace$, where $\\varepsilon$ is a timestep. The observation $O_t$ could be the last few image frames seen by the robot’s camera(s). The state $S_t$ and observation $O_t$ are both random variables. Let the state-observation pair be written as $X_t\\coloneq(S_t, O_t)$, which we will call the (augmented) state for simplicity. The $\\pi_{0.6}^\\ast$ paper refers to $X_t$ as the “observation”. The augmented state space is then $\\mathscr X\\coloneq\\mathscr S\\times \\mathscr O$.\nNote that the observation is different from the concept of a measurement in control theory — the observation does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. That is, the observation contains some useful information that even a perfect knowledge of the system state will not suffice to deduce. I will use the following notation to represent a family of random variables indexed by time, called a stochastic process: $$X_{[0,\\infty)}\\coloneq (X_s)_{s\\in[0,\\infty)}.$$ The family $X_{[0,\\infty)}$ is assumed to satisfy a Markov property: $$ p_{X_{t+s}| X_{[0,t]}}(x|\\chi)=p_{X_{t+s}|X_t}\\big(x|\\chi(t)\\big), $$ where $\\chi:\\mathbb [0,t] \\rightarrow \\mathscr X$ is a state trajectory. Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature. Firstly, the subscripts for “$p$” indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable/trajectory), and we are evaluating this pdf at $(x,\\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\\mathscr X$, making it clear that the above equality is assumed to hold for any point $x\\in\\mathscr X$. Similarly, it is assumed to hold for any trajectory $\\chi$.\nIn practice, we can augment the state/observation with additional information to ensure a Markov-like property.\n␥ Trajectories \u0026 Policies We can replace $[0,t]$ with $\\lbrace 0, \\varepsilon, \\cdots, {t-\\varepsilon}, t\\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\\mathscr X$ (or a section of a fiber bundle with base space the integers). So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\\chi)$ that should somehow “integrate to $1$”. What is its domain? One possibility is to consider the trivial fiber bundle $\\mathscr X \\times \\mathbb R\\rightarrow \\mathbb R$. The space of trajectories is then the space of sections of this bundle. If that means nothing to you, then we can simply write the trajectory-space as $C([0, t],\\mathscr X)$, suggesting (without much loss of generality) that the function $\\chi$ is continuous.\nThe goal of learning-based control is to learn a policy $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist designs and implements, e.g., using behavior cloning or RL. Let’s assume that the policy is time-invariant: $$\\pi_{A_t|X_t}=\\pi_{A_s|X_s}\\eqcolon \\pi$$ for all $t,s\\in[0,\\infty)$. We can always ‘cheat’ and add $t$ to the state to make this simplification hold.\nNow, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\\in[0,\\infty)}$, which is the random state-action trajectory. An example value that it can take is $(\\chi,\\alpha)$, where $\\chi:[0,\\infty)\\rightarrow \\mathscr X$ and $\\alpha:[0,\\infty)\\rightarrow \\mathscr A$. I reiterate that $(\\chi,\\alpha)$ is an example of a specific state-action trajectory, while $(X_t,A_t)$ is a random variable in $\\mathscr S\\times \\mathscr A$ representing the random state and action of the robot at time $t$.\nIn practice, the roboticist may only be able to specify $p_{A_{t+\\varepsilon}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\\varepsilon \u003e0$ amount of time, it is impossible to act at time $t$ based on the information at $t$. Do you (presumably a human) also operate on such a latency? In your case, $\\varepsilon$ is perhaps the time delay between when your Team Fortress 2 enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small! ␥ Reward Maximization Suppose the robot gets the instantaneous reward $R(x,a)$ for being in state $x$ and taking action $a$.\nSince we can change the policy, and the trajectory-distribution depends on the policy, we can change the trajectory-distribution $p_{X_{[0,\\infty)}}$.\n",
  "wordCount" : "774",
  "inLanguage": "en",
  "datePublished": "2025-12-06T18:56:12-05:00",
  "dateModified": "2025-12-06T18:56:12-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://10.0.0.26:1313/posts/rl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shiraz",
    "logo": {
      "@type": "ImageObject",
      "url": "http://10.0.0.26:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://10.0.0.26:1313/" accesskey="h" title="Shiraz (Alt + H)">Shiraz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://10.0.0.26:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://10.0.0.26:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="http://10.0.0.26:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      A Pedantic Introduction to VLA Models
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-12-06 18:56:12 -0500 EST'>December 6, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-states--observations" aria-label="␥ States &amp; Observations">␥ States &amp; Observations</a></li>
                <li>
                    <a href="#-trajectories--policies" aria-label="␥ Trajectories &amp; Policies">␥ Trajectories &amp; Policies</a></li>
                <li>
                    <a href="#-reward-maximization" aria-label="␥ Reward Maximization">␥ Reward Maximization</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="-states--observations"><span class=tertiary>␥</span> States &amp; Observations<a hidden class="anchor" aria-hidden="true" href="#-states--observations">#</a></h2>
<p>Let $\mathscr S$ be the state space of a robot, and $\mathscr O$ the observation space. The state $S_t$ might include robot&rsquo;s joint angles as measured/estimated during times $\lbrace t-\mathrm k \varepsilon:\mathrm k=0,1,\cdots,\mathrm n \rbrace$, where $\varepsilon$ is a timestep. The observation $O_t$ could be the last few image frames seen by the robot&rsquo;s camera(s). The state $S_t$ and observation $O_t$ are both <span class=accented>random variables</span>. Let the state-observation pair be written as $X_t\coloneq(S_t, O_t)$, which we will call the (augmented) state for simplicity. The <a href="https://www.physicalintelligence.company/blog/pistar06" target="_blank" class="accented">
    $\pi_{0.6}^\ast$ paper
</a> refers to $X_t$ as the &ldquo;observation&rdquo;. The augmented state space is then $\mathscr X\coloneq\mathscr S\times \mathscr O$.</p>
<aside class=aside-right>
Note that the <i>observation</i> is different from the concept of a <i>measurement</i>  in control theory — the observation does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. That is, the observation contains some useful information that even a perfect knowledge of the system state will not suffice to deduce.
</aside>
<p>I will use the following notation to represent a family of random variables indexed by time, called a <span class=accented>stochastic process</span>:
</p>
$$X_{[0,\infty)}\coloneq (X_s)_{s\in[0,\infty)}.$$<p>
The family $X_{[0,\infty)}$ is assumed to satisfy a Markov property:
</p>
$$
p_{X_{t+s}| X_{[0,t]}}(x|\chi)=p_{X_{t+s}|X_t}\big(x|\chi(t)\big),
$$<p>
where $\chi:\mathbb [0,t] \rightarrow \mathscr X$ is a state trajectory.
Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature.
Firstly, the subscripts for &ldquo;$p$&rdquo; indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable/trajectory), and we are evaluating this pdf at $(x,\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\mathscr X$, making it clear that the above equality is assumed to hold for <em>any</em> point $x\in\mathscr X$. Similarly, it is assumed to hold for any trajectory $\chi$.</p>
<p>In practice, we can augment the state/observation with additional information to ensure a Markov-like property.</p>
<h2 id="-trajectories--policies"><span class=tertiary>␥</span> Trajectories &amp; Policies<a hidden class="anchor" aria-hidden="true" href="#-trajectories--policies">#</a></h2>
<aside class=aside-right>
We can replace $[0,t]$ with $\lbrace 0, \varepsilon, \cdots, {t-\varepsilon}, t\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\mathscr X$ (or a section of a fiber bundle with base space the integers).
</aside>
<p>So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\chi)$ that should somehow &ldquo;integrate to $1$&rdquo;. What is its domain? One possibility is to consider the trivial <a href="/posts/bundles" class="accented">
    fiber bundle
</a> $\mathscr X \times \mathbb R\rightarrow \mathbb R$. The space of trajectories is then the space of <a href="http://staff.ustc.edu.cn/~wangzuoq/Courses/18F-Manifolds/Notes/Lec28.pdf" target="_blank" class="accented">
    sections
</a> of this bundle. If that means nothing to you, then we can simply write the trajectory-space as $C([0, t],\mathscr X)$, suggesting (without much loss of generality) that the function $\chi$ is continuous.</p>
<p>The goal of learning-based control is to learn a <span class=accented>policy</span> $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist <em>designs</em> and <em>implements</em>, e.g., using behavior cloning or RL. Let&rsquo;s assume that the policy is time-invariant: </p>
$$\pi_{A_t|X_t}=\pi_{A_s|X_s}\eqcolon \pi$$<p>
for all $t,s\in[0,\infty)$.
We can always &lsquo;cheat&rsquo; and add $t$ to the state to make this simplification hold.</p>
<p>Now, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\in[0,\infty)}$, which is the random state-action trajectory. An example value that it can take is $(\chi,\alpha)$, where $\chi:[0,\infty)\rightarrow \mathscr X$ and $\alpha:[0,\infty)\rightarrow \mathscr A$. I reiterate that $(\chi,\alpha)$ is an example of a specific state-action trajectory, while $(X_t,A_t)$ is a random variable in $\mathscr S\times \mathscr A$ representing the random state and action of the robot at time $t$.</p>
<aside class=aside-center>
In practice, the roboticist may only be able to specify $p_{A_{t+\varepsilon}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\varepsilon >0$ amount of time, it is impossible to act at time $t$ based on the information at $t$. Do you (presumably a human) also operate on such a latency? In your case, $\varepsilon$ is perhaps the time delay between when your <a href=https://www.teamfortress.com class=accented>Team Fortress 2</a> enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small!
</aside>
<h2 id="-reward-maximization"><span class=tertiary>␥</span> Reward Maximization<a hidden class="anchor" aria-hidden="true" href="#-reward-maximization">#</a></h2>
<p>Suppose the robot gets the instantaneous reward $R(x,a)$ for being in state $x$ and taking action $a$.</p>
<p>Since we can change the policy, and the trajectory-distribution depends on the policy, we can change the trajectory-distribution $p_{X_{[0,\infty)}}$.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://10.0.0.26:1313/tags/mechanics/">Mechanics</a></li>
      <li><a href="http://10.0.0.26:1313/tags/optimization/">Optimization</a></li>
      <li><a href="http://10.0.0.26:1313/tags/probability/">Probability</a></li>
    </ul>
  </footer>
<script src="https://giscus.app/client.js"
        data-repo="shirazkn/shirazkn.github.io"
        data-repo-id="R_kgDOI2VbWw"
        data-category="Announcements"
        data-category-id="DIC_kwDOI2VbW84CWJnt"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="noborder_light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    
    
    
    
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script>
if (localStorage.getItem("pref-theme") === "dark") {
    document.body.classList.add('dark');
}
</script>
<div class="headerfooter">
    <sub><sup><sub>..</sub></sup></sub>
</div>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            document.documentElement.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            document.documentElement.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
