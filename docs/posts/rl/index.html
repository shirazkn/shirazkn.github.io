<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>VLA Models | Shiraz</title>
<meta name="keywords" content="Mechanics, Optimization, Probability">
<meta name="description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/rl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.05929664bad1843e2a6ecdf2027ade7534ece873c5d97efdded31397c1d46881.css" integrity="sha256-BZKWZLrRhD4qbs3yAnredTTs6HPF2X793tMTl8HUaIE=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      packages: {'[+]': ['mathtools', 'amscd']},
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    },
    loader:{
      load: ['ui/safe', '[tex]/mathtools', '[tex]/amscd']
    },
  };
</script>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IM+Fell+DW+Pica">

<meta property="og:url" content="http://localhost:1313/posts/rl/">
  <meta property="og:site_name" content="Shiraz">
  <meta property="og:title" content="VLA Models">
  <meta property="og:description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:modified_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:tag" content="Mechanics">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Probability">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VLA Models">
<meta name="twitter:description" content="Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VLA Models",
      "item": "http://localhost:1313/posts/rl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VLA Models",
  "name": "VLA Models",
  "description": "Shiraz looks at the latest paper from physical intelligence, which should already be obsolete by the time this blog post gets done.",
  "keywords": [
    "Mechanics", "Optimization", "Probability"
  ],
  "articleBody": "␥ States \u0026 Observations Let $\\mathscr S$ be the state space of a robot, and $\\mathscr O$ the observation space. The state $S_t$ at time $t$ may be the robot’s joint angles as measured/estimated during times $$ t\\mathrel, \\,t-\\Delta t\\mathrel,\\, t-2\\,\\Delta t\\mathrel,\\, \\cdots\\mathrel,\\, t-(\\mathrm n - 1)\\,\\Delta t $$ where $\\Delta t$ is a timestep. The observation $O_t$ could be the last $\\mathrm n$ image frames seen by the robot’s camera(s). The state $S_t$ and observation $O_t$ are both random variables. Let the state-observation pair be written as $X_t\\coloneq(S_t, O_t)$, which we will simply call the “state” for simplicity. The $\\pi_{0.6}^\\ast$ paper refers to $X_t$ as the “observation”. The augmented state-space is $\\mathscr X\\coloneq\\mathscr S\\times \\mathscr O$.\nThe state $X_t$ does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. The goal of learning-based control is to learn a policy that maps $X_t$ to an action $A_t$, so that the robot can interact with (and respond to) its environment.\nI will use the following notation to represent a family of random variables indexed by time, called a stochastic process: $$ X_{[0,\\infty)}\\coloneq (X_s)_{s\\in[0,\\infty)} $$ The family $X_{[0,\\infty)}$ is assumed to satisfy a Markov property: $$ p_{X_{t+s}| X_{[0,t]}}(x|\\chi)=p_{X_{t+s}|X_t}\\big(x|\\chi(t)\\big), $$ where $\\chi: [0,t] \\rightarrow \\mathscr X$ is a state trajectory. Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature. Firstly, the subscripts for “$p$” indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable/trajectory), and we are evaluating this pdf at $(x,\\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\\mathscr X$, making it clear that the above equality is assumed to hold for any point $x\\in\\mathscr X$. Similarly, it is assumed to hold for any trajectory $\\chi$.\nWe can replace $[0,t]$ with $\\lbrace 0, \\Delta t, \\cdots, {t-\\Delta t}, t\\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\\mathscr X$ (or a section of a fiber bundle with base space the integers). In practice, we can augment the state with additional information to ensure a Markov-like property.\n␥ Trajectories \u0026 Policies So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\\chi)$ that should somehow “integrate to $1$”. What is its domain? To simplify things, we can write the trajectory-space as $C([0, t],\\mathscr X)$, suggesting (without much loss of generality) that the function $\\chi\\in C([0, t],\\mathscr X)$ is continuous.1\nThe goal of learning-based control is to learn a policy $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist designs and implements, e.g., using behavior cloning or RL. Let’s assume that the policy is time-invariant: $$\\pi_{A_t|X_t}=\\pi_{A_s|X_s}\\eqcolon \\pi$$ for all $t,s\\in[0,\\infty)$. We can always cheat and add $t$ to the state to make this simplification hold.\nNow, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\\in[0,\\infty)}$, which is the random state-action trajectory. If we have a dataset of state-action trajectories, we can learn the $\\pi$ that would generate them; this is behavior cloning. We can also make small perturbations to $\\pi$ to see if the resulting trajectories improve upon some reward function (in expectation); this is RL. Regularization can be introduced to ensure that the perturbed policy doesn’t deviate too far from some baseline policy (as done in TRPO and PPO). The regularization term is typically a divergence between $(X_t,A_t)_{t\\in[0,\\infty)}$ and the baseline policy.\n␥ Action Chunking Do you (presumably a human) also operate on such a latency? In your case, $\\delta$ is perhaps the time delay between when your Team Fortress 2 enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small! In practice, the roboticist may only be able to specify $p_{A_{t+\\delta}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\\delta \u003e0$ amount of time, it is impossible to act at time $t$ based on the information at $t$.2 Inference is expensive; if we inferenced at each (discretized) timestep, then the frequency of our outputs to the robot will be inherently limited by how fast we can do inference. The (currently) most well-known workaround is to ensure that each inference call produces a chunk of actions, i.e., a sequence of actions to be executed over the next $\\mathrm h$ timesteps, where $\\mathrm h$ is called the horizon. This is action chunking. In practice, it is common to execute only $\\mathrm e \u003c \\mathrm h$ actions from the chunk before requesting a new chunk of actions from the model, where $\\mathrm e \\approx \\mathrm h/2$.3\nHowever, what happens at the end of the chunk? There are two issues here:\nAfter executing a chunk, we still need to wait $\\delta$ milliseconds to get the next chunk of actions. Our robot hasn’t been told what to do during this time; if $\\delta \u003e \\Delta t$, then there will be a noticeable pause in the robot’s motion. Our VLA model doesn’t remember what it spat out during the last chunk, so the next chunk it produces will not continuously or smoothly align with the previous chunk. In my opinion, these aren’t very serious issues; as long as you have a robust VLA model, you can tell your robot to do the dishes and go to bed – the robot will do the dishes overnight in all of its jerky, jittery glory. However, these are serious issues for robotics labs because it makes for some seriously unimpressive demos. Recent papers from Physical Intelligence have attempted to address these issues.\nThe idea of real-time chunking (RTC) is to do the following. Let $\\mathrm d \\coloneq \\lfloor \\delta / \\Delta t \\rfloor$ be the number of timesteps that the model takes to do its inference, i.e., the inference delay. Suppose we already have this sequence of actions at timestep $t$: $$ a_{t}, a_{t + \\Delta t}, \\cdots, a_{t + (\\mathrm h-1) \\Delta t} $$ Then, in two parallel threads, do the following:\nexecute the first $\\mathrm e$ actions in the chunk as $a_{t + \\mathrm e \\Delta t}$ is being executed, initiate another call for inference; condition this inference on the remaining $\\mathrm d$ actions that will be executed while the inference is happening.[^inpainting] The conditioning step can be viewed as freezing the first $\\mathrm d$ actions (copied from the previous chunk) and inpainting the rest of the action chunk, similar to how image inpainting works. This way, we have new chunk of actions waiting for us at time $\\mathrm e$, and this new chunk will (due to the conditioning) smoothly continue from the previous chunk. The robot doesn’t have to stop moving, and the chunks are smoothly stitched together.\nThe paper I linked above is actually better-described as inference-time RTC . It uses a gradient-based inpainting method to fill in the missing actions; basically, the score/velocity vector in the diffusion/flow-matching process incurs an additional term calculated via a vector-Jacobian product (VJP), which ensures that the denoised/flow-matched chunk begins with the $\\mathrm d$ frozen actions.\nTraining-time RTC is concerned with the problem that the gradient-based inpainting is expensive. Their solution is to simply not denoise the first $\\mathrm d$ actions during inference\nAnother possibility is to consider the trivial fiber bundle $\\mathscr X \\times \\mathbb R\\rightarrow \\mathbb R$. The space of trajectories is then the space of sections of this bundle. ↩︎\nThe OpenVLA has $\\delta \\approx 320\\,ms$ after accounting for model inference, network latency, and other overheads. ↩︎\nIf only $\\mathrm e \u003c \\mathrm h$ actions are executed, then why do we predict the remaining $\\mathrm h - \\mathrm e$ actions at all? My understanding is that this encourages the model to think long-term (a la model-predictive control) rather than resorting to a greedy policy. ↩︎\n",
  "wordCount" : "1371",
  "inLanguage": "en",
  "datePublished": "2025-12-06T18:56:12-05:00",
  "dateModified": "2025-12-06T18:56:12-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/rl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shiraz",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Shiraz (Alt + H)">Shiraz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      VLA Models
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-12-06 18:56:12 -0500 EST'>December 6, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-states--observations" aria-label="␥ States &amp; Observations">␥ States &amp; Observations</a></li>
                <li>
                    <a href="#-trajectories--policies" aria-label="␥ Trajectories &amp; Policies">␥ Trajectories &amp; Policies</a></li>
                <li>
                    <a href="#-action-chunking" aria-label="␥ Action Chunking">␥ Action Chunking</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="-states--observations"><span class=tertiary>␥</span> States &amp; Observations<a hidden class="anchor" aria-hidden="true" href="#-states--observations">#</a></h2>
<p>Let $\mathscr S$ be the state space of a robot, and $\mathscr O$ the observation space. The state $S_t$ at time $t$ may be the robot&rsquo;s joint angles as measured/estimated during times
</p>
$$
t\mathrel, \,t-\Delta t\mathrel,\, t-2\,\Delta t\mathrel,\, \cdots\mathrel,\, t-(\mathrm n - 1)\,\Delta t
$$<p>
where $\Delta t$ is a timestep. The observation $O_t$ could be the last $\mathrm n$ image frames seen by the robot&rsquo;s camera(s). The state $S_t$ and observation $O_t$ are both <span class=accented>random variables</span>. Let the state-observation pair be written as $X_t\coloneq(S_t, O_t)$, which we will simply call the &ldquo;state&rdquo; for simplicity. The <a href="https://www.physicalintelligence.company/blog/pistar06" target="_blank" class="accented">
    $\pi_{0.6}^\ast$ paper
</a> refers to $X_t$ as the &ldquo;observation&rdquo;. The augmented state-space is $\mathscr X\coloneq\mathscr S\times \mathscr O$.</p>
<p>The <i>state</i> $X_t$ does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. The goal of learning-based control is to learn a policy that maps $X_t$ to an action $A_t$, so that the robot can interact with (and respond to) its environment.</p>
<p>I will use the following notation to represent a family of random variables indexed by time, called a <span class=accented>stochastic process</span>:
</p>
$$
X_{[0,\infty)}\coloneq (X_s)_{s\in[0,\infty)}
$$<p>
The family $X_{[0,\infty)}$ is assumed to satisfy a Markov property:
</p>
$$
p_{X_{t+s}| X_{[0,t]}}(x|\chi)=p_{X_{t+s}|X_t}\big(x|\chi(t)\big),
$$<p>
where $\chi: [0,t] \rightarrow \mathscr X$ is a state trajectory.
Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature.
Firstly, the subscripts for &ldquo;$p$&rdquo; indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable/trajectory), and we are evaluating this pdf at $(x,\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\mathscr X$, making it clear that the above equality is assumed to hold for <em>any</em> point $x\in\mathscr X$. Similarly, it is assumed to hold for any trajectory $\chi$.</p>
<aside class=aside-right>
We can replace $[0,t]$ with $\lbrace 0, \Delta t, \cdots, {t-\Delta t}, t\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\mathscr X$ (or a section of a fiber bundle with base space the integers).
</aside>
<p>In practice, we can augment the state with additional information to ensure a Markov-like property.</p>
<h2 id="-trajectories--policies"><span class=tertiary>␥</span> Trajectories &amp; Policies<a hidden class="anchor" aria-hidden="true" href="#-trajectories--policies">#</a></h2>
<p>So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\chi)$ that should somehow &ldquo;integrate to $1$&rdquo;. What is its domain? To simplify things, we can write the trajectory-space as $C([0, t],\mathscr X)$, suggesting (without much loss of generality) that the function $\chi\in C([0, t],\mathscr X)$ is continuous.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>The goal of learning-based control is to learn a <span class=accented>policy</span> $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist <em>designs</em> and <em>implements</em>, e.g., using behavior cloning or RL. Let&rsquo;s assume that the policy is time-invariant: </p>
$$\pi_{A_t|X_t}=\pi_{A_s|X_s}\eqcolon \pi$$<p>
for all $t,s\in[0,\infty)$.
We can always cheat and add $t$ to the state to make this simplification hold.</p>
<p>Now, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\in[0,\infty)}$, which is the random state-action trajectory. If we have a dataset of state-action trajectories, we can learn the $\pi$ that would generate them; this is behavior cloning. We can also make small perturbations to $\pi$ to see if the resulting trajectories improve upon some reward function (in expectation); this is RL. Regularization can be introduced to ensure that the perturbed policy doesn&rsquo;t deviate too far from some baseline policy (as done in TRPO and PPO). The regularization term is typically a <a href="https://en.wikipedia.org/wiki/Divergence_%28statistics%29" target="_blank" class="accented">
    divergence
</a> between $(X_t,A_t)_{t\in[0,\infty)}$ and the baseline policy.</p>
<h2 id="-action-chunking"><span class=tertiary>␥</span> Action Chunking<a hidden class="anchor" aria-hidden="true" href="#-action-chunking">#</a></h2>
<aside class=aside-right>
Do you (presumably a human) also operate on such a latency? In your case, $\delta$ is perhaps the time delay between when your <a href=https://www.teamfortress.com class=accented>Team Fortress 2</a> enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small!
</aside>
<p>In practice, the roboticist may only be able to specify $p_{A_{t+\delta}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\delta >0$ amount of time, it is impossible to act at time $t$ based on the information at $t$.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Inference is expensive; if we inferenced at each (discretized) timestep, then the frequency of our outputs to the robot will be inherently limited by how fast we can do inference. The (currently) most well-known workaround is to ensure that each inference call produces a <span class=accented>chunk</span> of actions, i.e., a sequence of actions to be executed over the next $\mathrm h$ timesteps, where $\mathrm h$ is called the <span class=accented>horizon</span>. This is <span class=accented>action chunking</span>. In practice, it is common to execute only $\mathrm e < \mathrm h$ actions from the chunk before requesting a new chunk of actions from the model, where $\mathrm e \approx \mathrm h/2$.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>However, what happens at the end of the chunk? There are two issues here:</p>
<ol>
<li>After executing a chunk, we still need to wait $\delta$ milliseconds to get the next chunk of actions. Our robot hasn&rsquo;t been told what to do during this time; if $\delta > \Delta t$, then there will be a noticeable pause in the robot&rsquo;s motion.</li>
<li>Our VLA model doesn&rsquo;t remember what it spat out during the last chunk, so the next chunk it produces will not continuously or smoothly align with the previous chunk.</li>
</ol>
<p>In my opinion, these aren&rsquo;t very serious issues; as long as you have a robust VLA model, you can tell your robot to do the dishes and go to bed &ndash; the robot will do the dishes overnight in all of its jerky, jittery glory. However, these <em>are</em> serious issues for robotics labs because it makes for some seriously unimpressive demos. Recent papers from <a href="https://www.pi.website" target="_blank" class="accented">
    Physical Intelligence
</a> have attempted to address these issues.</p>
<p>The idea of <a href="https://arxiv.org/pdf/2506.07339" target="_blank" class="accented">
    real-time chunking (RTC)
</a> is to do the following. Let $\mathrm d \coloneq \lfloor \delta / \Delta t \rfloor$ be the number of timesteps that the model takes to do its inference, i.e., the <em>inference delay</em>.
Suppose we already have this sequence of actions at timestep $t$:
</p>
$$
a_{t}, a_{t + \Delta t}, \cdots, a_{t + (\mathrm h-1) \Delta t}
$$<p>
Then, in two parallel threads, do the following:</p>
<ul>
<li>execute the first $\mathrm e$ actions in the chunk</li>
<li>as $a_{t + \mathrm e \Delta t}$ is being executed, initiate another call for inference; condition this inference on the remaining $\mathrm d$ actions that will be executed while the inference is happening.[^inpainting]</li>
</ul>
<p>The conditioning step can be viewed as <span class=accented>freezing</span> the first $\mathrm d$ actions (copied from the previous chunk) and <em>inpainting</em> the rest of the action chunk, similar to how image inpainting works. This way, we have new chunk of actions waiting for us at time $\mathrm e$, and this new chunk will (due to the conditioning) smoothly continue from the previous chunk. The robot doesn&rsquo;t have to stop moving, and the chunks are smoothly stitched together.</p>
<p>The paper I linked above is actually better-described as <a href="https://arxiv.org/pdf/2506.07339" target="_blank" class="accented">
    inference-time RTC
</a>. It uses a gradient-based inpainting method to fill in the missing actions; basically, the score/velocity vector in the diffusion/flow-matching process incurs an additional term calculated via a vector-Jacobian product (VJP), which ensures that the denoised/flow-matched chunk begins with the $\mathrm d$ frozen actions.</p>
<p><a href="https://arxiv.org/pdf/2512.05964" target="_blank" class="accented">
    Training-time RTC
</a> is concerned with the problem that the gradient-based inpainting is expensive. Their solution is to simply not denoise the first $\mathrm d$ actions during inference</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Another possibility is to consider the trivial <a href="/posts/bundles" class="accented">
    fiber bundle
</a> $\mathscr X \times \mathbb R\rightarrow \mathbb R$. The space of trajectories is then the space of <a href="http://staff.ustc.edu.cn/~wangzuoq/Courses/18F-Manifolds/Notes/Lec28.pdf" target="_blank" class="accented">
    sections
</a> of this bundle.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The OpenVLA has $\delta \approx 320\,ms$ after accounting for model inference, network latency, and other overheads.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>If only $\mathrm e < \mathrm h$ actions are executed, then why do we predict the remaining $\mathrm h - \mathrm e$ actions at all? My understanding is that this encourages the model to think long-term (<em>a la</em> model-predictive control) rather than resorting to a greedy policy.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/mechanics/">Mechanics</a></li>
      <li><a href="http://localhost:1313/tags/optimization/">Optimization</a></li>
      <li><a href="http://localhost:1313/tags/probability/">Probability</a></li>
    </ul>
  </footer>
<script src="https://giscus.app/client.js"
        data-repo="shirazkn/shirazkn.github.io"
        data-repo-id="R_kgDOI2VbWw"
        data-category="Announcements"
        data-category-id="DIC_kwDOI2VbW84CWJnt"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="noborder_light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    
    
    
    
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script>
if (localStorage.getItem("pref-theme") === "dark") {
    document.body.classList.add('dark');
}
</script>
<div class="headerfooter">
    <sub><sup><sub>..</sub></sup></sub>
</div>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            document.documentElement.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            document.documentElement.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
