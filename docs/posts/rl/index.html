<!DOCTYPE html>
<html lang="en" dir="auto" data-theme="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>VLA Models | Shiraz</title>
<meta name="keywords" content="Mechanics, Optimization, Probability">
<meta name="description" content="I look at some of the central ideas that go into designing and training VLA models, using the latest papers by Physical Intelligence as reference.">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/posts/rl/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.5f72b9d6274026476ca7b83fe1a6a6a203c09a251266869341599852ddb17f19.css" integrity="sha256-X3K51idAJkdsp7g/4aamogPAmiUSZoaTQVmYUt2xfxk=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/favicon.ico">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rl/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
                color-scheme: dark;
            }

            .list {
                background: var(--theme);
            }

            .toc {
                background: var(--entry);
            }
        }

        @media (prefers-color-scheme: light) {
            .list::-webkit-scrollbar-thumb {
                border-color: var(--code-bg);
            }
        }

    </style>
</noscript>
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.querySelector("html").dataset.theme = 'dark';
    } else if (localStorage.getItem("pref-theme") === "light") {
       document.querySelector("html").dataset.theme = 'light';
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.querySelector("html").dataset.theme = 'dark';
    } else {
        document.querySelector("html").dataset.theme = 'light';
    }

</script><script>
(function() {
    var pref = localStorage.getItem("pref-theme");
    if (pref === "dark" || (!pref && window.matchMedia("(prefers-color-scheme: dark)").matches)) {
        document.documentElement.classList.add("dark");
    }
})();
</script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      packages: {'[+]': ['mathtools', 'amscd']},
      displayMath: [['\\[', '\\]'], ['$$', '$$']],  
      inlineMath: [['\\(', '\\)'], ['$', '$']]  
    },
    loader:{
      load: ['ui/safe', '[tex]/mathtools', '[tex]/amscd']
    },
  };
</script>

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=IM+Fell+DW+Pica">

<meta property="og:url" content="http://localhost:1313/posts/rl/">
  <meta property="og:site_name" content="Shiraz">
  <meta property="og:title" content="VLA Models">
  <meta property="og:description" content="I look at some of the central ideas that go into designing and training VLA models, using the latest papers by Physical Intelligence as reference.">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:modified_time" content="2025-12-06T18:56:12-05:00">
    <meta property="article:tag" content="Mechanics">
    <meta property="article:tag" content="Optimization">
    <meta property="article:tag" content="Probability">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="VLA Models">
<meta name="twitter:description" content="I look at some of the central ideas that go into designing and training VLA models, using the latest papers by Physical Intelligence as reference.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "VLA Models",
      "item": "http://localhost:1313/posts/rl/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "VLA Models",
  "name": "VLA Models",
  "description": "I look at some of the central ideas that go into designing and training VLA models, using the latest papers by Physical Intelligence as reference.",
  "keywords": [
    "Mechanics", "Optimization", "Probability"
  ],
  "articleBody": "␥ States \u0026 Observations Let $\\mathscr S$ be the state space of a robot and $\\mathscr O$ the observation space. The state at time $t$ is written as $S_t$, which is an $\\mathscr S$-valued random variable. For example, $S_t$ may be the robot’s joint angles at times $$ \\begin{array}{c@{\\,,\\ }c@{\\,,\\ }c@{\\,,\\ }c@{\\,,\\ }c@{}c} t-(\\mathrm n - 1)\\tau \u0026 \\cdots \\,\u0026 t-2\\tau \u0026 t-\\tau \u0026 t \\end{array} $$ where $\\tau$ is a timestep. In this example, $\\mathscr S=(SO(2))^{\\mathrm j\\mathrm n}$, where $\\mathrm j$ the number of joints and $\\mathrm n$ is the number of timesteps. The observation $O_t$ could be the last $\\mathrm n$ image frames seen by the robot’s camera(s). The state $S_t$ and observation $O_t$ are both random variables. Let the state-observation pair be written as $X_t\\coloneq(S_t, O_t)$, which we will simply call the “state” for simplicity. The $\\pi_{0.6}^\\ast$ paper refers to $X_t$ as the “observation”. The augmented state-space is $\\mathscr X\\coloneq\\mathscr S\\times \\mathscr O$.\nThe state $X_t$ does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. The goal of learning-based control is to learn a mapping that takes $X_t$ and outputs an action $A_t$, enabling the robot to interact with (and respond to) its environment.\nI will use the following notation to represent a family of random variables indexed by time, called a stochastic process: $$ X_{[0,\\infty)}\\coloneq (X_s)_{s\\in[0,\\infty)} $$ The family $X_{[0,\\infty)}$ is assumed to satisfy a Markov property: $$ p_{X_{t+s}| X_{[0,t]}}(x|\\chi)=p_{X_{t+s}|X_t}\\big(x|\\chi(t)\\big), $$ where $\\chi: [0,t] \\rightarrow \\mathscr X$ is a state trajectory. Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature. Firstly, the subscripts for “$p$” indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable), and we are evaluating this pdf at $(x,\\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\\mathscr X$, making it clear that the above equality is assumed to hold for any point $x\\in\\mathscr X$. Similarly, it is assumed to hold for any trajectory $\\chi$. In practice, we can augment the state with additional information to ensure a Markov-like property.\nWe can replace $[0,t]$ with $\\lbrace 0, \\tau, \\cdots, {t-\\tau}, t\\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\\mathscr X$ (or a section of a fiber bundle with base space the integers). ␥ Trajectories \u0026 Policies So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\\chi)$ that should somehow “integrate to $1$”. What is its domain? To simplify things, we can write the trajectory-space as $C([0, t],\\mathscr X)$, assuming without much loss of generality that the trajectory $\\chi\\in C([0, t],\\mathscr X)$ is continuous.1\nThe goal of learning-based control is to learn a policy $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist designs and implements, e.g., using behavior cloning or RL. Let’s assume that the policy is time-invariant: $$p_{A_t|X_t}=p_{A_s|X_s}\\eqcolon \\pi$$ for all $t,s\\in[0,\\infty)$. We can always cheat and add $t$ to the state to make this simplification hold.\nNow, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\\in[0,\\infty)}$, which is the random state-action trajectory. Conversely, if we have a dataset of state-action trajectories, we can learn the $\\pi$ that would generate them; this is behavior cloning. We can also make small perturbations to $\\pi$ to see if the resulting trajectories improve upon some reward function (in expectation); this is reinforcement learning. Regularization can be introduced to ensure that the perturbed policy doesn’t deviate too far from some baseline policy (as done in TRPO and PPO). The regularization term is typically an information-theoretic divergence between $(X_t,A_t)_{t\\in[0,\\infty)}$ and the baseline policy; the divergence between two distributions measures how much they differ from each other, statistically.\n␥ Action Chunking Do you (presumably a human) also operate on such a latency? In your case, $\\delta$ is perhaps the time delay between when your Team Fortress 2 enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small! In practice, the roboticist may only be able to specify $p_{A_{t+\\delta}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\\delta \u003e0$ amount of time, it is impossible to act at time $t$ based on the information at $t$.2 Inference is expensive; if we inferenced at each (discretized) timestep, then the frequency of our outputs to the robot will be inherently limited by how fast we can do inference. The (currently) most well-known workaround is to ensure that each inference call produces a chunk of actions, i.e., a sequence of actions to be executed over the next $\\mathrm h$ timesteps, where $\\mathrm h$ is called the horizon. This is action chunking. In practice, it is common to execute only $\\mathrm e \u003c \\mathrm h$ actions from the chunk before requesting a new chunk of actions from the model, where $\\mathrm e \\approx \\mathrm h/2$.3\nHowever, what happens at the end of the chunk? There are two issues here:\nAfter executing a chunk, we still need to wait $\\delta$ milliseconds to get the next chunk of actions. Our robot hasn’t been told what to do during this time; if $\\delta \u003e \\tau$, then there will be a noticeable pause in the robot’s motion. Our VLA model doesn’t remember what it spat out during the last chunk, so the next chunk it produces will not continuously or smoothly align with the previous chunk. In my opinion, these aren’t very serious issues; as long as you have a robust VLA model you can tell your robot to do the dishes and go to bed – the robot will do the dishes overnight in all of its jerky, jittery glory. However, these are serious issues for robotics labs like Physical Intelligence because it makes for some seriously unimpressive demos. Recent papers from Physical Intelligence have attempted to address these issues.\nThe idea of real-time chunking (RTC) is to do the following. Let $\\mathrm d \\coloneq \\lfloor \\delta / \\tau \\rfloor$ be the number of timesteps that the model takes to do its inference, i.e., the inference delay. Suppose we already have this sequence of actions at timestep $t$: $$ \\begin{array}{ccccc} a_{t} \u0026 a_{t + \\tau} \u0026 a_{t + 2\\tau} \u0026 \\cdots \u0026 a_{t + (\\mathrm h-1) \\tau} \\end{array} $$ Then, in two parallel threads, do the following (assuming $\\mathrm h\\geq \\mathrm e + \\mathrm d$):\nexecute the first $\\mathrm e$ actions in the chunk as $a_{t + \\mathrm e \\tau}$ is being executed, initiate another call for inference; condition this inference on the next $\\mathrm d$ actions in the chunk: $$ \\begin{array}{cccc} a_{t+\\mathrm e \\tau} \u0026 a_{t +(\\mathrm e + 1)\\tau} \u0026 \\cdots \u0026 a_{t + (\\mathrm e+\\mathrm d - 1) \\tau} \\end{array} $$ as these actions will be executed while the inference is happening! The conditioning step can be viewed as freezing the next $\\mathrm d$ actions of the previous chunk and inpainting the rest of the action chunk, using the same algorithms as those used for image inpainting. This way, we have new chunk of actions waiting for us at time $t+(\\mathrm e+\\mathrm d)\\tau$, and this new chunk will (due to the conditioning) smoothly continue from the previous chunk. The robot doesn’t have to stop moving, and the chunks are smoothly stitched together.\nThe paper I linked above is actually better-described as inference-time RTC . It uses a gradient-based inpainting method to fill in the missing actions. Basically, the score (or velocity) vector in the diffusion (or flow-matching) process incurs an additional term calculated via a vector-Jacobian product, ensuring that the denoised (or flow-matched) chunk begins with the $\\mathrm d$ frozen actions. So “conditioning” here refers to the gradient-based correction term.\nTraining-time RTC is concerned with the problem that the gradient-based inpainting is expensive. Their solution is to simply not denoise the first $\\mathrm d$ actions during inference, but to pass these as observations to the denoising process (e.g., as part of the FiLM conditioning module). So the main difference is in how the conditioning is done. The adjectives inference-time vs. training-time is a bit misleading; both algorithms require some inference-time modifications over the vanilla diffusion (or flow matching) VLA policy, but IT-RTC doesn’t require any training-time changes.\nAnother possibility is to consider the trivial fiber bundle $\\mathscr X \\times \\mathbb R\\rightarrow \\mathbb R$. The space of trajectories is then the space of sections of this bundle. ↩︎\nOpenVLA has $\\delta \\approx 320\\,ms$ after accounting for model inference, network latency, and other overheads. ↩︎\nIf only $\\mathrm e \u003c \\mathrm h$ actions are executed, then why do we predict the remaining $\\mathrm h - \\mathrm e$ actions at all? My understanding is that this encourages the model to think long-term (a la model-predictive control) rather than resorting to a greedy policy. ↩︎\n",
  "wordCount" : "1545",
  "inLanguage": "en",
  "datePublished": "2025-12-06T18:56:12-05:00",
  "dateModified": "2025-12-06T18:56:12-05:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/rl/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shiraz",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>
<body id="top">
    <header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Shiraz (Alt + H)">Shiraz</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/archives/" title="archives">
                    <span>archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="tags">
                    <span>tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="search (Alt &#43; /)" accesskey=/>
                    <span>search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      VLA Models
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-12-06 18:56:12 -0500 EST'>December 6, 2025</span>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#-states--observations" aria-label="␥ States &amp; Observations">␥ States &amp; Observations</a></li>
                <li>
                    <a href="#-trajectories--policies" aria-label="␥ Trajectories &amp; Policies">␥ Trajectories &amp; Policies</a></li>
                <li>
                    <a href="#-action-chunking" aria-label="␥ Action Chunking">␥ Action Chunking</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="-states--observations"><span class=tertiary>␥</span> States &amp; Observations<a hidden class="anchor" aria-hidden="true" href="#-states--observations">#</a></h2>
<p>Let $\mathscr S$ be the state space of a robot and $\mathscr O$ the observation space. The state at time $t$ is written as $S_t$, which is an $\mathscr S$-valued random variable. For example, $S_t$ may be the robot&rsquo;s joint angles at times
</p>
$$
\begin{array}{c@{\,,\ }c@{\,,\ }c@{\,,\ }c@{\,,\ }c@{}c}
t-(\mathrm n - 1)\tau & \cdots \,&  t-2\tau & t-\tau & t
\end{array}
$$<p>
where $\tau$ is a timestep. In this example, $\mathscr S=(SO(2))^{\mathrm j\mathrm n}$, where $\mathrm j$ the number of joints and $\mathrm n$ is the number of timesteps. The observation $O_t$ could be the last $\mathrm n$ image frames seen by the robot&rsquo;s camera(s). The state $S_t$ and observation $O_t$ are both <span class=accented>random variables</span>. Let the state-observation pair be written as $X_t\coloneq(S_t, O_t)$, which we will simply call the &ldquo;state&rdquo; for simplicity. The <a href="https://www.physicalintelligence.company/blog/pistar06" target="_blank" class="accented">
    $\pi_{0.6}^\ast$ paper
</a> refers to $X_t$ as the &ldquo;observation&rdquo;. The augmented state-space is $\mathscr X\coloneq\mathscr S\times \mathscr O$.</p>
<p>The <i>state</i> $X_t$ does not merely represent noisy/partial information about our robot, but also includes information about the environment and objects that our robot interacts with. The goal of learning-based control is to learn a mapping that takes $X_t$ and outputs an action $A_t$, enabling the robot to interact with (and respond to) its environment.</p>
<p>I will use the following notation to represent a family of random variables indexed by time, called a <span class=accented>stochastic process</span>:
</p>
$$
X_{[0,\infty)}\coloneq (X_s)_{s\in[0,\infty)}
$$<p>
The family $X_{[0,\infty)}$ is assumed to satisfy a Markov property:
</p>
$$
p_{X_{t+s}| X_{[0,t]}}(x|\chi)=p_{X_{t+s}|X_t}\big(x|\chi(t)\big),
$$<p>
where $\chi: [0,t] \rightarrow \mathscr X$ is a state trajectory.
Maybe this sick-looking notation unsettles you, but I find it to be much cleaner than what is typically used in the reinforcement learning (RL) literature.
Firstly, the subscripts for &ldquo;$p$&rdquo; indicate that two different functions are used on either side — on the left, and we are evaluating the conditional pdf of $X_{t+s}$ (a random variable) conditioned on $X_{[0,t]}$ (a random variable), and we are evaluating this pdf at $(x,\chi)$. Secondly, $X_t$ is a random variable while $x$ represents an arbitrary point in $\mathscr X$, making it clear that the above equality is assumed to hold for <em>any</em> point $x\in\mathscr X$. Similarly, it is assumed to hold for any trajectory $\chi$. In practice, we can augment the state with additional information to ensure a Markov-like property.</p>
<aside class=aside-right>
We can replace $[0,t]$ with $\lbrace 0, \tau, \cdots, {t-\tau}, t\rbrace$ to recover the discrete-time formulation. For instance, a discrete-time trajectory may be viewed as a function from the integers to $\mathscr X$ (or a section of a fiber bundle with base space the integers).
</aside>
<h2 id="-trajectories--policies"><span class=tertiary>␥</span> Trajectories &amp; Policies<a hidden class="anchor" aria-hidden="true" href="#-trajectories--policies">#</a></h2>
<p>So, $X_{[0,t]}$ is a random state trajectory (from $0$ to $t$), and $\chi$ is an example of a value that it can take. This implies the existence of a pdf $p_{X_{[0,t]}}(\chi)$ that should somehow &ldquo;integrate to $1$&rdquo;. What is its domain? To simplify things, we can write the trajectory-space as $C([0, t],\mathscr X)$, assuming without much loss of generality that the trajectory $\chi\in C([0, t],\mathscr X)$ is continuous.<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>The goal of learning-based control is to learn a <span class=accented>policy</span> $p_{A_t|X_t}(a|x)$ — a conditional pdf that specifies the probability density of the robot taking action $a$ given that it is at the state $x$. This is something that a roboticist <em>designs</em> and <em>implements</em>, e.g., using behavior cloning or RL. Let&rsquo;s assume that the policy is time-invariant: </p>
$$p_{A_t|X_t}=p_{A_s|X_s}\eqcolon \pi$$<p>
for all $t,s\in[0,\infty)$.
We can always cheat and add $t$ to the state to make this simplification hold.</p>
<p>Now, suppose there is some underlying distribution of initial conditions (represented by $X_0$) and we fix a policy $\pi$, then we get a combined stochastic process $(X_t,A_t)_{t\in[0,\infty)}$, which is the random state-action trajectory. Conversely, if we have a dataset of state-action trajectories, we can learn the $\pi$ that would generate them; this is <span class=accented>behavior cloning</span>. We can also make small perturbations to $\pi$ to see if the resulting trajectories improve upon some reward function (in expectation); this is <span class=accented>reinforcement learning</span>. Regularization can be introduced to ensure that the perturbed policy doesn&rsquo;t deviate too far from some baseline policy (as done in TRPO and PPO). The regularization term is typically an information-theoretic <a href="https://en.wikipedia.org/wiki/Divergence_%28statistics%29" target="_blank" class="accented">
    divergence
</a> between $(X_t,A_t)_{t\in[0,\infty)}$ and the baseline policy; the divergence between two distributions measures how much they differ from each other, statistically.</p>
<h2 id="-action-chunking"><span class=tertiary>␥</span> Action Chunking<a hidden class="anchor" aria-hidden="true" href="#-action-chunking">#</a></h2>
<aside class=aside-right>
Do you (presumably a human) also operate on such a latency? In your case, $\delta$ is perhaps the time delay between when your <a href=https://www.teamfortress.com class=accented>Team Fortress 2</a> enemy first appears on-screen to when you begin moving the reticle towards their head. It is impressively small!
</aside>
<p>In practice, the roboticist may only be able to specify $p_{A_{t+\delta}|X_t}(a|x)$ due to latency issues! Since doing model inference takes an $\delta >0$ amount of time, it is impossible to act at time $t$ based on the information at $t$.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Inference is expensive; if we inferenced at each (discretized) timestep, then the frequency of our outputs to the robot will be inherently limited by how fast we can do inference. The (currently) most well-known workaround is to ensure that each inference call produces a <span class=accented>chunk</span> of actions, i.e., a sequence of actions to be executed over the next $\mathrm h$ timesteps, where $\mathrm h$ is called the <span class=accented>horizon</span>. This is <span class=accented>action chunking</span>. In practice, it is common to execute only $\mathrm e < \mathrm h$ actions from the chunk before requesting a new chunk of actions from the model, where $\mathrm e \approx \mathrm h/2$.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>However, what happens at the end of the chunk? There are two issues here:</p>
<ol>
<li>After executing a chunk, we still need to wait $\delta$ milliseconds to get the next chunk of actions. Our robot hasn&rsquo;t been told what to do during this time; if $\delta > \tau$, then there will be a noticeable pause in the robot&rsquo;s motion.</li>
<li>Our VLA model doesn&rsquo;t remember what it spat out during the last chunk, so the next chunk it produces will not continuously or smoothly align with the previous chunk.</li>
</ol>
<p>In my opinion, these aren&rsquo;t very serious issues; as long as you have a robust VLA model you can tell your robot to do the dishes and go to bed &ndash; the robot will do the dishes overnight in all of its jerky, jittery glory. However, these <em>are</em> serious issues for robotics labs like <a href="https://www.pi.website" target="_blank" class="accented">
    Physical Intelligence
</a> because it makes for some seriously unimpressive demos. Recent papers from Physical Intelligence have attempted to address these issues.</p>
<p>The idea of <a href="https://www.pi.website/research/real_time_chunking" target="_blank" class="accented">
    real-time chunking (RTC)
</a> is to do the following. Let $\mathrm d \coloneq \lfloor \delta / \tau \rfloor$ be the number of timesteps that the model takes to do its inference, i.e., the <em>inference delay</em>.
Suppose we already have this sequence of actions at timestep $t$:
</p>
$$
\begin{array}{ccccc}
a_{t} & a_{t + \tau} & a_{t + 2\tau} & \cdots & a_{t + (\mathrm h-1) \tau}
\end{array}
$$<p>
Then, in two parallel threads, do the following (assuming $\mathrm h\geq \mathrm e + \mathrm d$):</p>
<ul>
<li>execute the first $\mathrm e$ actions in the chunk</li>
<li>as $a_{t + \mathrm e \tau}$ is being executed, initiate another call for inference; condition this inference on the next $\mathrm d$ actions in the chunk:
$$
  \begin{array}{cccc}
  a_{t+\mathrm e \tau} & a_{t +(\mathrm e + 1)\tau} & \cdots & a_{t + (\mathrm e+\mathrm d - 1) \tau}
  \end{array}
  $$
as these actions will be executed while the inference is happening!</li>
</ul>
<p>The conditioning step can be viewed as <span class=accented>freezing</span> the next $\mathrm d$ actions of the previous chunk and <em>inpainting</em> the rest of the action chunk, using the same algorithms as those used for image inpainting. This way, we have new chunk of actions waiting for us at time $t+(\mathrm e+\mathrm d)\tau$, and this new chunk will (due to the conditioning) smoothly continue from the previous chunk. The robot doesn&rsquo;t have to stop moving, and the chunks are smoothly stitched together.</p>
<p>The paper I linked above is actually better-described as <a href="https://www.pi.website/research/real_time_chunking" target="_blank" class="accented">
    inference-time RTC
</a>. It uses a gradient-based inpainting method to fill in the missing actions. Basically, the score <span class=tertiary>(or velocity)</span> vector in the diffusion <span class=tertiary>(or flow-matching)</span> process incurs an additional term calculated via a vector-Jacobian product, ensuring that the denoised <span class=tertiary>(or flow-matched)</span> chunk begins with the $\mathrm d$ frozen actions. So &ldquo;conditioning&rdquo; here refers to the gradient-based correction term.</p>
<p><a href="https://arxiv.org/pdf/2512.05964" target="_blank" class="accented">
    Training-time RTC
</a> is concerned with the problem that the gradient-based inpainting is expensive. Their solution is to simply not denoise the first $\mathrm d$ actions during inference, but to pass these as observations to the denoising process (e.g., as part of the <a href="https://arxiv.org/abs/1709.07871" target="_blank" class="accented">
    FiLM conditioning
</a> module). So the main difference is in how the conditioning is done. The adjectives inference-time vs. training-time is a bit misleading; both algorithms require some inference-time modifications over the vanilla diffusion <span class=tertiary>(or flow matching)</span> VLA policy, but IT-RTC doesn&rsquo;t require any training-time changes.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Another possibility is to consider the trivial <a href="/posts/bundles" class="accented">
    fiber bundle
</a> $\mathscr X \times \mathbb R\rightarrow \mathbb R$. The space of trajectories is then the space of <a href="http://staff.ustc.edu.cn/~wangzuoq/Courses/18F-Manifolds/Notes/Lec28.pdf" target="_blank" class="accented">
    sections
</a> of this bundle.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>OpenVLA has $\delta \approx 320\,ms$ after accounting for model inference, network latency, and other overheads.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>If only $\mathrm e < \mathrm h$ actions are executed, then why do we predict the remaining $\mathrm h - \mathrm e$ actions at all? My understanding is that this encourages the model to think long-term (<em>a la</em> model-predictive control) rather than resorting to a greedy policy.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/mechanics/">Mechanics</a></li>
      <li><a href="http://localhost:1313/tags/optimization/">Optimization</a></li>
      <li><a href="http://localhost:1313/tags/probability/">Probability</a></li>
    </ul>
  </footer>
<script src="https://giscus.app/client.js"
        data-repo="shirazkn/shirazkn.github.io"
        data-repo-id="R_kgDOI2VbWw"
        data-category="Announcements"
        data-category-id="DIC_kwDOI2VbW84CWJnt"
        data-mapping="title"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="bottom"
        data-theme="noborder_light"
        data-lang="en"
        crossorigin="anonymous"
        async>
</script>
</article>
    </main>
    
<footer class="footer">
    
    
    
    
    
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a><script>

function syncDarkClass() {
    const isDark = document.documentElement.dataset.theme === 'dark';
    document.documentElement.classList.toggle('dark', isDark);
    document.body.classList.toggle('dark', isDark);
}
syncDarkClass();


const themeToggle = document.getElementById('theme-toggle');
if (themeToggle) {
    themeToggle.addEventListener('click', () => {
        setTimeout(syncDarkClass, 0);
    });
}


document.querySelectorAll('.logo a, .post-title').forEach(el => {
    el.innerHTML = el.textContent.trim().split(' ').map(word =>
        `<span class="hover-word">${word.split('').map(char =>
            `<span class="hover-letter">${char}</span>`
        ).join('')}</span>`
    ).join(' ');
});


const isDark = () => document.body.classList.contains('dark');
const hoverColors = isDark()
    ? ['#ff51af', '#ff51af', '#f161ac', '#f161ac', '#db327e', '#db327e', '#3b82f6', '#38bdf8']
    : ['#ff51af', '#ff51af', '#e83a89', '#e83a89', '#f542a1', '#d6306f', '#3b9eff', '#00c2ff'];
document.querySelectorAll('.hover-letter').forEach(letter => {
    letter.addEventListener('mouseenter', () => {
        letter.style.color = hoverColors[Math.floor(Math.random() * hoverColors.length)];
    });
    letter.addEventListener('mouseleave', () => {
        letter.style.color = '';
    });
});


const scanElement = (el) => {
    const letters = el.querySelectorAll('.hover-letter');
    letters.forEach((letter, i) => {
        setTimeout(() => {
            letter.style.transition = 'top 0.05s ease-out, color 0s';
            letter.style.color = hoverColors[Math.floor(Math.random() * hoverColors.length)];
            letter.style.top = '-0.08em';
            setTimeout(() => {
                letter.style.transition = '';
                letter.style.color = '';
                letter.style.top = '';
            }, 100);
        }, i * 50);
    });
};

let lastScannedEl = null;
const triggerRandomScan = () => {
    const logos = Array.from(document.querySelectorAll('.logo a'));
    const titles = Array.from(document.querySelectorAll('.post-title'));
    
    const pickLogo = Math.random() < 0.7 && logos.length > 0;
    let elements = pickLogo ? logos : titles;
    
    if (lastScannedEl && elements.length > 1) {
        elements = elements.filter(el => el !== lastScannedEl);
    }
    if (elements.length > 0) {
        const el = elements[Math.floor(Math.random() * elements.length)];
        lastScannedEl = el;
        scanElement(el);
    }
    
    setTimeout(triggerRandomScan, 4000 + Math.random() * 6000);
};


setTimeout(triggerRandomScan, 5000);
</script>
<div class="headerfooter">
    <sub><sup><sub>..</sub></sup></sub>
</div>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            document.documentElement.dataset.theme = 'light';
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            document.documentElement.dataset.theme = 'dark';
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
