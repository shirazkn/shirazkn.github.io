<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Shiraz</title>
    <link>https://shirazkn.github.io/</link>
    <description>Recent content on Shiraz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Apr 2023 17:15:26 -0400</lastBuildDate><atom:link href="https://shirazkn.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Sparsity through Sub-Gradients</title>
      <link>https://shirazkn.github.io/posts/sparsity_2/</link>
      <pubDate>Fri, 28 Apr 2023 17:15:26 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/sparsity_2/</guid>
      <description>&lt;!-- This post will require some familiarity with optimization (or least-squares, if you will). --&gt;
&lt;p&gt;&lt;a href=&#34;https://shirazkn.github.io/posts/sparsity&#34; class=&#34;accented&#34;&gt;
    We talked about
&lt;/a&gt; why sparsity plays an important role in many of the &lt;a href=&#34;https://en.wikipedia.org/wiki/Inverse_problem&#34; target=&#34;_blank&#34; class=&#34;accented&#34;&gt;
    inverse problems
&lt;/a&gt; that we encounter in engineering. To actually find the sparse solutions to these problems, we add &amp;lsquo;sparsity-promoting&amp;rsquo; terms to our optimization problems; the machine learning community calls this approach &lt;em&gt;regularization&lt;/em&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Sparsity</title>
      <link>https://shirazkn.github.io/posts/sparsity/</link>
      <pubDate>Sat, 22 Apr 2023 11:05:58 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/sparsity/</guid>
      <description>The so called curse of dimensionality in machine learning is the observation that neural networks with many parameters can be impossibly difficult to train due to the vastness of its parameter space. Another issue that arises in practice is that most of the neural network does not do anything, as a lot of its weights turn out to be redundant. This is because many (if not all) of the problems we&amp;rsquo;re interested in solving as engineers have some inherent sparsity.</description>
    </item>
    
    <item>
      <title>Hilbert Spaces</title>
      <link>https://shirazkn.github.io/posts/hilbert-spaces/</link>
      <pubDate>Fri, 21 Apr 2023 12:09:09 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/hilbert-spaces/</guid>
      <description>Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&amp;rsquo;t have er&amp;hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product.</description>
    </item>
    
    <item>
      <title>Norm Balls</title>
      <link>https://shirazkn.github.io/posts/balls/</link>
      <pubDate>Tue, 18 Apr 2023 21:32:10 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/balls/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s look at the norm balls corresponding to the different $p$-norms in $\mathbb R^n$, where $n$ is the dimension of the space. For a vector $v\in \mathbb R^n$, the $p$-norm is&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Real World is a Special Case</title>
      <link>https://shirazkn.github.io/posts/pythagoras/</link>
      <pubDate>Fri, 14 Apr 2023 15:53:46 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/pythagoras/</guid>
      <description>The title is a quote from this math podcast . I mentioned in the last post that Euclidean geometry arises as a special case of the geometry of inner product spaces. And yet, the only spaces that are &amp;rsquo;tangible&amp;rsquo; to us humans are the $1$, $2$, and $3$ dimensional Euclidean spaces. No other inner product spaces feel nearly as intuitive.
In this post I&amp;rsquo;m showing how the Pythagoras theorem is a special case of a more general feature of inner product spaces.</description>
    </item>
    
    <item>
      <title>Norms, Metrics, and Inner Products</title>
      <link>https://shirazkn.github.io/posts/norms_metrics/</link>
      <pubDate>Mon, 10 Apr 2023 12:20:23 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/norms_metrics/</guid>
      <description>This is an explainer on norms, metrics, and inner products, and their relationships to each other.
Norms A norm is any real-valued function $\lVert{}\cdot{}\rVert$ (taking the elements of a corresponding vector space as its arguments), which has the following properties:
It is nonnegative, and $0$ only at the &amp;lsquo;zero element&amp;rsquo; (for e.g., at the origin of $\mathbb R^n$).
$\lVert \alpha x \rVert = |\alpha| \lVert x \rVert$ for any scalar $\alpha$.</description>
    </item>
    
    <item>
      <title>The Unreasonable Effectiveness of $2$ in Statistics</title>
      <link>https://shirazkn.github.io/posts/leastsquares/</link>
      <pubDate>Sun, 09 Apr 2023 12:20:39 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/leastsquares/</guid>
      <description>First off, this is a reference to The Unreasonable Effectiveness of Mathematics in the Natural Sciences , a very popular paper by Eugene Wigner which explores how mathematics is unreasonably effective at not only explaining, but also predicting scientific phenomena. I had a similar question about the number $2$ which repeatedly shows up in engineering and science, specifically in the form of the $2$-norm of a vector, and seems surprisingly effective at doing what it&amp;rsquo;s supposed to do.</description>
    </item>
    
    <item>
      <title>Cat Theory ðŸ˜¼</title>
      <link>https://shirazkn.github.io/posts/cat_theory_1/</link>
      <pubDate>Sat, 18 Mar 2023 21:07:38 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/cat_theory_1/</guid>
      <description>One of my motivations for starting a blog was Eugenia Cheng&amp;rsquo;s book The Joy of Abstraction 1. It&amp;rsquo;s a surprisingly accessible, gentle introduction to category theory, a topic that is usually only taught to graduate students in math. She compiled part of the book using notes from the category theory class that she teaches at the Art Institute of Chicago, a testament to the aesthetic appreciation that one can expect to gain of category theory, irrespective of their academic background.</description>
    </item>
    
    <item>
      <title>The Incompleteness Theorems</title>
      <link>https://shirazkn.github.io/posts/language_and_logic2/</link>
      <pubDate>Sun, 05 Feb 2023 08:46:18 -0500</pubDate>
      
      <guid>https://shirazkn.github.io/posts/language_and_logic2/</guid>
      <description>In my earlier post I suggested that there is no objective notion of logical truth, that whether a statement is &amp;rsquo;true&amp;rsquo; can depend on the system of truth that one is operating in. Here we will develop that argument further using the concept of axiomatic systems. This is a long one, but I&amp;rsquo;m excited to talk about it!
Axiomatic Systems An axiomatization is an assignment of rules (axioms) such as &amp;ldquo;one plus one equals two,&amp;rdquo; making up an axiomatic system or a formal system.</description>
    </item>
    
    <item>
      <title>Misuse as a Use of Language</title>
      <link>https://shirazkn.github.io/posts/language_and_logic1/</link>
      <pubDate>Fri, 27 Jan 2023 13:48:49 -0500</pubDate>
      
      <guid>https://shirazkn.github.io/posts/language_and_logic1/</guid>
      <description>This is my first post! It discusses the question of whether spoken and written languages like English could be &amp;rsquo;logical&amp;rsquo; by design. I will break this post up into two parts. The first one does not require a mathematical background whatsoever, whereas the second touches on the concepts of axioms, theorems and proofs.
Fallacies The word logic, as it is used in everyday parlance, refers to informal logic (as opposed to formal logic, which is instead a rigorous mathematical construct; we will look at formal logic in the sequel).</description>
    </item>
    
    
  </channel>
</rss>
