<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linear Algebra on Shiraz</title>
    <link>https://shirazkn.github.io/tags/linear-algebra/</link>
    <description>Recent content in Linear Algebra on Shiraz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Apr 2023 11:05:58 -0400</lastBuildDate><atom:link href="https://shirazkn.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sparsity</title>
      <link>https://shirazkn.github.io/posts/sparsity/</link>
      <pubDate>Sat, 22 Apr 2023 11:05:58 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/sparsity/</guid>
      <description>The so called curse of dimensionality in machine learning is the observation that neural networks with many parameters can be impossibly difficult to train due to the vastness of its parameter space. Another issue that arises in practice is that most of the neural network does not do anything, as a lot of its weights turn out to be redundant. This is because many (if not all) of the problems we&amp;rsquo;re interested in solving as engineers have some inherent sparsity.</description>
    </item>
    
    <item>
      <title>Hilbert Spaces</title>
      <link>https://shirazkn.github.io/posts/hilbert-spaces/</link>
      <pubDate>Fri, 21 Apr 2023 12:09:09 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/hilbert-spaces/</guid>
      <description>Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&amp;rsquo;t have er&amp;hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product for vectors.</description>
    </item>
    
    <item>
      <title>Norm Balls</title>
      <link>https://shirazkn.github.io/posts/balls/</link>
      <pubDate>Tue, 18 Apr 2023 21:32:10 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/balls/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s look at the norm balls corresponding to the different $p$-norms in $\mathbb R^n$, where $n$ is the dimension of the space. For a vector $v\in \mathbb R^n$, the $p$-norm is&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Real World is a Special Case</title>
      <link>https://shirazkn.github.io/posts/pythagoras/</link>
      <pubDate>Fri, 14 Apr 2023 15:53:46 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/pythagoras/</guid>
      <description>The title is a quote from this math podcast . I mentioned in the last post that Euclidean geometry arises as a special case of the geometry of inner product spaces. And yet, the only spaces that are &amp;rsquo;tangible&amp;rsquo; to us humans are the $1$, $2$, and $3$ dimensional Euclidean spaces. No other inner product spaces feel nearly as intuitive.
In this post I&amp;rsquo;m showing how the Pythagoras theorem is a special case of a more general feature of inner product spaces.</description>
    </item>
    
    <item>
      <title>Norms, Metrics, and Inner Products</title>
      <link>https://shirazkn.github.io/posts/norms_metrics/</link>
      <pubDate>Mon, 10 Apr 2023 12:20:23 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/norms_metrics/</guid>
      <description>This is an explainer on norms, metrics, and inner products, and their relationships to each other.
Norms A norm is any real-valued function $\lVert{}\cdot{}\rVert$ (taking the elements of a corresponding vector space as its arguments), which has the following properties:
It is nonnegative, and $0$ only at the &amp;lsquo;zero element&amp;rsquo; (for e.g., at the origin of $\mathbb R^n$).
$\lVert \alpha x \rVert = |\alpha| \lVert x \rVert$ for any scalar $\alpha$.</description>
    </item>
    
    <item>
      <title>The Unreasonable Effectiveness of $2$ in Statistics</title>
      <link>https://shirazkn.github.io/posts/leastsquares/</link>
      <pubDate>Sun, 09 Apr 2023 12:20:39 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/leastsquares/</guid>
      <description>First off, this is a reference to The Unreasonable Effectiveness of Mathematics in the Natural Sciences , a very popular paper by Eugene Wigner which explores how mathematics is unreasonably effective at not only explaining, but also predicting scientific phenomena. I had a similar question about the number $2$ which repeatedly shows up in engineering and science, specifically in the form of the $2$-norm of a vector, and seems surprisingly effective at doing what it&amp;rsquo;s supposed to do.</description>
    </item>
    
  </channel>
</rss>
