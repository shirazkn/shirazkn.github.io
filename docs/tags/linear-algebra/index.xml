<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Linear Algebra on Shiraz</title>
    <link>https://shirazkn.github.io/tags/linear-algebra/</link>
    <description>Recent content in Linear Algebra on Shiraz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Jan 2024 11:22:28 -0500</lastBuildDate><atom:link href="https://shirazkn.github.io/tags/linear-algebra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Vector Fields on Manifolds</title>
      <link>https://shirazkn.github.io/posts/differential-forms/</link>
      <pubDate>Wed, 03 Jan 2024 11:22:28 -0500</pubDate>
      
      <guid>https://shirazkn.github.io/posts/differential-forms/</guid>
      <description>Over the past year, I have struggled to pin down what the scope of my blog should be. There is plenty of exposition out there on just about every aspect of modern mathematics, but especially on exterior calculus and differential geometry due to their situation at the intersection of several areas in theoretical and applied mathematics. (As a case in point, the two main references that I&amp;rsquo;ve been using to self-learn differential forms were the creations of a theoretical physicist and a computer scientist , respectively).</description>
    </item>
    
    <item>
      <title>Matrix Multiplication</title>
      <link>https://shirazkn.github.io/posts/matrix/</link>
      <pubDate>Sun, 28 May 2023 10:35:07 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/matrix/</guid>
      <description>In this post, I want to bridge the gap between abstract vector spaces (which are the mathematical foundation of linear algebra) and matrix multiplication (which is the linear algebra most of us are familiar with). To do this, we will restrict ourselves to a specific example of a vector space &amp;ndash; the Euclidean space. Unlike the typical 101 course in linear algebra, I will avoid talking about solving systems of equations in this post.</description>
    </item>
    
    <item>
      <title>What is a Vector?</title>
      <link>https://shirazkn.github.io/posts/vector/</link>
      <pubDate>Sat, 20 May 2023 15:26:18 -0700</pubDate>
      
      <guid>https://shirazkn.github.io/posts/vector/</guid>
      <description>A running gag in engineering colleges is that a lot of instructors begin their first class of the semester with this question: &amp;ldquo;What is a vector?&amp;rdquo;. I used to find this ritual almost pointless because to me, every answer to this question felt either like a non-answer or a matter of context. I mean it depends, right? A structural engineer should have a different answer to this question than, say, a data scientist.</description>
    </item>
    
    <item>
      <title>Sparsity</title>
      <link>https://shirazkn.github.io/posts/sparsity/</link>
      <pubDate>Sat, 22 Apr 2023 11:05:58 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/sparsity/</guid>
      <description>The so called curse of dimensionality in machine learning is the observation that neural networks with many parameters can be impossibly difficult to train due to the vastness of its parameter space. Another issue that arises in practice is that most of the neural network does not do anything, as a lot of its weights turn out to be redundant. This is because many (if not all) of the problems we&amp;rsquo;re interested in solving as engineers have some inherent sparsity.</description>
    </item>
    
    <item>
      <title>Hilbert Spaces</title>
      <link>https://shirazkn.github.io/posts/hilbert-spaces/</link>
      <pubDate>Fri, 21 Apr 2023 12:09:09 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/hilbert-spaces/</guid>
      <description>Let $\mathcal X$ be a Hilbert space, which means that it is a vector space that has an inner product (denoted by $\langle \cdot, \cdot\rangle _\mathcal X$) and that it is complete, i.e., it doesn&amp;rsquo;t have er&amp;hellip; holes in it. Recall that inner product spaces have a rich geometric structure, and so do Hilbert spaces. The Euclidean space $\mathbb R^n$ is an obvious example, where the inner product is just the dot product.</description>
    </item>
    
    <item>
      <title>Norm Balls</title>
      <link>https://shirazkn.github.io/posts/balls/</link>
      <pubDate>Tue, 18 Apr 2023 21:32:10 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/balls/</guid>
      <description>&lt;p&gt;Let&amp;rsquo;s look at the norm balls corresponding to the different $p$-norms in $\mathbb R^n$, where $n$ is the dimension of the space. For a vector $v\in \mathbb R^n$, the $p$-norm is&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>The Real World is a Special Case</title>
      <link>https://shirazkn.github.io/posts/pythagoras/</link>
      <pubDate>Fri, 14 Apr 2023 15:53:46 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/pythagoras/</guid>
      <description>The title is a quote from this math podcast . I mentioned in the last post that Euclidean geometry arises as a special case of the geometry of inner product spaces. And yet, the only spaces that are &amp;rsquo;tangible&amp;rsquo; to us humans are the $1$, $2$, and $3$ dimensional Euclidean spaces. No other inner product spaces feel nearly as intuitive.
In this post I&amp;rsquo;m showing how the Pythagoras theorem is a special case of a more general feature of inner product spaces.</description>
    </item>
    
    <item>
      <title>Norms, Metrics, and Inner Products</title>
      <link>https://shirazkn.github.io/posts/norms_metrics/</link>
      <pubDate>Mon, 10 Apr 2023 12:20:23 -0400</pubDate>
      
      <guid>https://shirazkn.github.io/posts/norms_metrics/</guid>
      <description>This is an explainer on norms, metrics, and inner products, and their relationships to each other.
Norms A norm is any real-valued function $\lVert{}\cdot{}\rVert$ (taking the elements of a corresponding vector space as its arguments), which has the following properties:
It is nonnegative, and $0$ only at the &amp;lsquo;zero element&amp;rsquo; (for e.g., at the origin of $\mathbb R^n$).
$\lVert \alpha x \rVert = |\alpha| \lVert x \rVert$ for any scalar $\alpha$.</description>
    </item>
    
  </channel>
</rss>
