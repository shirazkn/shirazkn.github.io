<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Optimization on Shiraz</title>
    <link>https://shirazkn.github.io/tags/optimization/</link>
    <description>Recent content in Optimization on Shiraz</description>
    <generator>Hugo -- 0.154.5</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Apr 2023 17:15:26 -0400</lastBuildDate>
    <atom:link href="https://shirazkn.github.io/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Understanding Sparsity through Sub-Gradients</title>
      <link>https://shirazkn.github.io/posts/sparsity_2/</link>
      <pubDate>Fri, 28 Apr 2023 17:15:26 -0400</pubDate>
      <guid>https://shirazkn.github.io/posts/sparsity_2/</guid>
      <description>We talked about why sparsity plays an important role in many of the inverse problems that we encounter in engineering. To actually find the sparse solutions to these problems, we add &amp;lsquo;sparsity-promoting&amp;rsquo; terms to our optimization problems; the machine learning community calls this approach regularization.</description>
    </item>
    <item>
      <title>Sparsity</title>
      <link>https://shirazkn.github.io/posts/sparsity/</link>
      <pubDate>Sat, 22 Apr 2023 11:05:58 -0400</pubDate>
      <guid>https://shirazkn.github.io/posts/sparsity/</guid>
      <description>The so called curse of dimensionality in machine learning is the observation that neural networks with many parameters can be impossibly difficult to train due to the vastness of its parameter space. This is because many (if not all) of the problems we&amp;rsquo;re interested in solving as engineers have some inherent sparsity.</description>
    </item>
    <item>
      <title>Norm Balls</title>
      <link>https://shirazkn.github.io/posts/balls/</link>
      <pubDate>Tue, 18 Apr 2023 21:32:10 -0400</pubDate>
      <guid>https://shirazkn.github.io/posts/balls/</guid>
      <description>Let&amp;rsquo;s look at the norm balls corresponding to the different p-norms. When p equals 2 this is the usual Euclidean distance. The corresponding ball is what we think of when someone says &amp;lsquo;ball&amp;rsquo;, it is all the points that are within a given distance from the origin.</description>
    </item>
    <item>
      <title>The Unreasonable Effectiveness of &#39;2&#39; in Statistics</title>
      <link>https://shirazkn.github.io/posts/leastsquares/</link>
      <pubDate>Sun, 09 Apr 2023 12:20:39 -0400</pubDate>
      <guid>https://shirazkn.github.io/posts/leastsquares/</guid>
      <description>The title is a reference to The Unreasonable Effectiveness of Mathematics in the Natural Sciences. I had a similar question about the number 2 which repeatedly shows up in engineering and science, specifically in the form of the 2-norm of a vector, and seems surprisingly effective at doing what it&amp;rsquo;s supposed to do.</description>
    </item>
  </channel>
</rss>
