<!-- ($1$) has a surprisingly useful visual interpretation as well. -->

<!-- to add: 
does language follow logic? statistically maybe, entropy (3b1b), tai-danae's paper, monte carlo, 
... but then anything with even the slightest amount of regularity is mathematical.
incompleteness theorems as languages.. -->

<!-- however, languages are informal, allow statements like this sentence is false. -->
<!-- Aside: would a language that is expressive enough to encapsulate mathematical logic be incomplete?
language allows us to transcend mathematical systems. -->
<!-- Takashi Tokeida's paradox about propositional logic (contravariance) -->


## Emmy Noether

## David Blackwell

## Alan Turing

## Maryam Mirzakhani

## Hilda Geiringer

## Honorable Mentions

### Bell Burnell

### Vera Rubin

### Srinivasa Ramanujan

### Sophie Germain, Ada Lovelace, Hypatia, Florence Nightingale

## In our time...

### Eugenia Cheng

### Terrence Tao

---
title: "Decategorification of Music, Math, and the Human Condition"
date: 2023-08-13T10:27:23-04:00
draft: true
---

*Decategorification*, a word whose conception is attributed to John Baez, means the opposite of what it might sound like. What it sounds like is that we're picking objects out of a *category* and examining what it is; what it really means is that we treat objects that are somewhat equal (in a certain sense) as being equal. When someone says `red`, if inside your head flash images of apples, traffic lights, and blood in quick succession, then you have decategorified the notion of `red`. Similarly, $2$ is the decategorification of everything that occurs or exists in a pair: birds (sometimes), socks (if you do your laundry right), hands (in most people). I guess what we're really doing is `categorizing' these concepts into the `category' `$2$', but oh well.

<!-- Category theory is a branch of math that constructs a common type of object, called a *category*, for each type of object. This allows us to learn new things in one category, and apply it to another. We do this all the time. 

In the category of sets, isomorphic  -->

Isomorphic objects are those that have an *isomorphism* between them, which in turn implies that they 'have the same form'; *iso* means same, and *morphism* means form. An isomorphism allows us to go back and forth between the objects without ambiguity, whereas a *homomorphism* only allows us to go in one direction (usually because it forgets or ignores some of the information about the source object).

The act of taking a photograph is a homomorphism from a person to an image of them. We can take photographs of people, but we can't use (a single) photograph to reconstruct the person, even just a $3D$ model of them. This is because we've forgotten their depth; maybe they have a tail on their bottom, and the photograph was taken from the front so that we may never know. We can *infer* that they do not, as with most humans, have a tail. This is what humans do in engineering to `fill in the gaps', and it's what AI does. The human capability of inference comes from our lived and taught experiences, as well as genetic and evolutionary predispositions. AI scrapes the internet.

An isomorphism allows us to go both ways. An MRI scan is (if we choose to squint at its blemishes) an isomorphism between the physical characteristics of human brain and a digital representation of it. Note that I said isomorphism *between* (and homomorphism *from*) to emphasize the symmetry (and asymmetry) of these concepts.



---
title: "The Integers"
date: 2023-06-08T10:06:51-04:00
draft: true
---

The numbers $0$ and $1$ play special roles in the arithmetic of integers. I will elaborate on that, and also talk about why they occur right next to each other on the number line.

$$ \begin{array}\end{array}

### What are the Integers?

Addition and multiplication are *binary operations*. While defining the integers, we need to 
The integers are a set of objects which have two binary operations




---
title: "The SVD Cheatsheet"
date: 2023-06-04T10:10:35-04:00
draft: true
---

### Preliminaries

**Matrices**: Suppose there is a function (also called a transformation or a map), 
$$A:\mathbb R^n \rightarrow \mathbb R^m$$ 
that takes every vector in $\mathbb R^n$ to a vector in (a subspace of) $\mathbb R^m$. Let's also suppose that this function is <span class=accented>linear</span>, which means that

$$\begin{align*}
A(\mathbf u &+ \mathbf v) = A(\mathbf u) + A(\mathbf v)\\\
\textit{\\&}\qquad \ &
A(k \mathbf v) = k A(\mathbf v)
\end{align*}$$
where $\mathbf u, \mathbf v \in \mathbb R^n$ and $k\in \mathbb R$. Then we can represent $A(\mathbf v)$ via matrix-multiplication, written as $\mathbf A \mathbf v$. There is (with some [caveats](/posts/matrix)) a unique $n\times m$ matrix $\mathbf A$ that represents a given linear transformation, $A$. 

**Image**: The image of $\mathbf A$ is the set of values it actually maps the vectors in $\mathbb R^n$ to, denoted as $\text{Im}(\mathbf A) \subseteq \mathbb R^m$. While the codomain of $\mathbf A$ (which is $\mathbb R^m$) is $m$-dimensional, the dimension of $\text{Im}(\mathbf A)$ may be smaller than $m$.
This is because $\mathbf A$ might map some subspaces of $\mathbb R^n$ to $\mathbf 0$: 

<div>
<figure class=invertible style="max-width: 100%;">
<img src=/post-images/linear_algebra/singular_mapping.png>
</figure>
</div>

<!-- The union of the subspaces that are mapped to $\mathbf 0$ under $\mathbf A$ is called the *null space* of $\mathbf A$, denoted as $\text{Null}(\mathbf A)$.  -->

The dimension of $\text{Im}(\mathbf A)$ is called as the <span class=accented>$\text{Rank}$</span> of $\mathbf A$.

At the same time, $\text{Im}(\mathbf A)$ cannot have dimension greater than $n$. Mapping to a lower-dimensional vector space is akin to '<span class=accented>losing information</span>' (which is possible), whereas mapping to a higher-dimensional vector space is akin to '<span class=accented>creating information out of nowhere</span>' (which is not possible). In the diagram shown above, we 'lost' the information about the height of the cylinder while doing the transformation.

**Orthonormal Matrices**: Suppose we have a set of vectors 

$$\mathcal B=\lbrace \mathbf b_1 \mathbf b_2, \dots, \mathbf b_n \rbrace$$

which form a [basis](/posts/matrix) for $\mathbb R^n$. Then, we can uniquely express a vector $\mathbf v\in \mathbb R^n$ as a linear combination:

$$\mathbf v = \sum_{i=1}^n v_i \mathbf b_i$$

In addition, if $\mathbf b_1, \mathbf b_2, \dots, \mathbf b_n$ are [orthonormal](/posts/matrix), then we have

$$\langle b_i